{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlcmsYP8iukI",
        "outputId": "b8ca7f61-1c9e-4560-c4a6-3fa75d5a50f5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J-AZ71k0Iau",
        "outputId": "5395b8e8-eee6-4ed2-9242-4943795b4a3c"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2defrC721Xa",
        "outputId": "9ca9f7f9-b296-40b1-b9e3-699080b2cfa3"
      },
      "outputs": [],
      "source": [
        "!mkdir -p weights\n",
        "!wget -nc https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EXLKQiPfv4K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnGSFiwxz97n",
        "outputId": "5ed181fb-9673-4589-e246-700bde436655"
      },
      "outputs": [],
      "source": [
        "from mobile_sam import sam_model_registry, SamPredictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRlFQEZY4Ogo"
      },
      "source": [
        "## Evaluation Dataset class\n",
        "\n",
        "Since MobileSAM is a promptable segmentation model, we need to pass some prompts (bounding box in this case) into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z7pbVOT5c6K"
      },
      "outputs": [],
      "source": [
        "# Standard ImageNet Normalization\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rQnK1IWhXPC"
      },
      "outputs": [],
      "source": [
        "class EvaluationPromptableSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_bounding_boxes_from_mask(self, mask, padding_factor=0.1, min_area_threshold=5):\n",
        "        \"\"\"Get one or multiple bounding boxes from binary mask.\"\"\"\n",
        "        _, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "        boxes = []\n",
        "        for i in range(1, num_labels):\n",
        "            x, y, w, h, area = (\n",
        "                stats[i, cv2.CC_STAT_LEFT],\n",
        "                stats[i, cv2.CC_STAT_TOP],\n",
        "                stats[i, cv2.CC_STAT_WIDTH],\n",
        "                stats[i, cv2.CC_STAT_HEIGHT],\n",
        "                stats[i, cv2.CC_STAT_AREA],\n",
        "            )\n",
        "            if area < min_area_threshold:\n",
        "                continue\n",
        "\n",
        "            pad = int(max(w, h) * padding_factor)\n",
        "            x_min, y_min = max(0, x - pad), max(0, y - pad)\n",
        "            x_max, y_max = min(mask.shape[1], x + w + pad), min(mask.shape[0], y + h + pad)\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = self.masks[idx]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Load image and mask\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        original_H, original_W = mask.shape\n",
        "        boxes = self.get_bounding_boxes_from_mask(mask)\n",
        "\n",
        "        # Resize\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "        if mask.shape[:2] != self.img_size:\n",
        "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Image normalization\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        mask_tensor = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n",
        "\n",
        "        # Return all boxes, not just one (SAM can handle multiple)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"mask\": mask_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2acFp7riW9G"
      },
      "source": [
        "**Note:** Please replace your own test set paths here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cOlJjtWehXRV"
      },
      "outputs": [],
      "source": [
        "test_sets_base_path = \"/content/drive/MyDrive/FYP/Datasets/\"\n",
        "test_set_1_images_path = os.path.join(test_sets_base_path, \"FUSeg/test/images\")\n",
        "test_set_1_masks_path = os.path.join(test_sets_base_path, \"FUSeg/test/labels\")\n",
        "\n",
        "test_set_2_images_path = os.path.join(test_sets_base_path, \"DFUC2022/test/images\")\n",
        "test_set_2_masks_path = os.path.join(test_sets_base_path, \"DFUC2022/test/masks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GixG9MrkA8-6"
      },
      "outputs": [],
      "source": [
        "test_sets = {\n",
        "    \"test_set_FUSeg\": EvaluationPromptableSegmentationDataset(test_set_1_images_path, test_set_1_masks_path),\n",
        "    \"test_set_DFUC2022\": EvaluationPromptableSegmentationDataset(test_set_2_images_path, test_set_2_masks_path),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq96e7eoq3Y"
      },
      "source": [
        "### Helper function to get the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB03lc9Losyq"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix_components(y_true, y_pred, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates the confusion matrix components (TP, FP, FN) for a batch.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground truth masks, a tensor of 0s and 1s.\n",
        "        y_pred (torch.Tensor): Predicted masks from the model, a tensor of continuous values.\n",
        "        threshold (float): The binarization threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing True Positives, False Positives, False Negatives, and True Negatives.\n",
        "    \"\"\"\n",
        "    # Binarize predictions using the specified threshold\n",
        "    y_pred = (y_pred > threshold).float()\n",
        "\n",
        "    # Flatten tensors for easier calculation\n",
        "    y_true_flat = y_true.view(-1)\n",
        "    y_pred_flat = y_pred.view(-1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    true_positives = ((y_pred_flat == 1) & (y_true_flat == 1)).sum().item()\n",
        "    false_positives = ((y_pred_flat == 1) & (y_true_flat == 0)).sum().item()\n",
        "    false_negatives = ((y_pred_flat == 0) & (y_true_flat == 1)).sum().item()\n",
        "    true_negatives = ((y_pred_flat == 0) & (y_true_flat == 0)).sum().item()\n",
        "\n",
        "    return true_positives, false_positives, false_negatives, true_negatives\n",
        "\n",
        "# Final metrics calculation function\n",
        "def calculate_final_metrics(tp, fp, fn, tn, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Calculates final metrics from accumulated confusion matrix components.\n",
        "    \"\"\"\n",
        "    # IoU\n",
        "    intersection = tp\n",
        "    union = tp + fp + fn\n",
        "    iou = intersection / (union + smooth)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * tp) / (2 * tp + fp + fn + smooth)\n",
        "\n",
        "    # Recall (Sensitivity)\n",
        "    recall = tp / (tp + fn + smooth)\n",
        "\n",
        "    # Precision (Positive Predictive Value)\n",
        "    precision = tp / (tp + fp + smooth)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + smooth)\n",
        "\n",
        "    return iou, dice, recall, precision, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdqjM27UznYt"
      },
      "source": [
        "## MobileSAM + Domain Adapter Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bh-6zKu28Oh"
      },
      "outputs": [],
      "source": [
        "class DecoderAdapter(nn.Module):\n",
        "  \"\"\"\n",
        "  A bottleneck adapter module for PEFT.\n",
        "  Inserts a small trainable module into the mask decoder.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim: int, adapter_dim: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # Down-projection: from model dimension (in_dim) to a smaller adapter_dim\n",
        "    self.down = nn.Linear(in_dim, adapter_dim)\n",
        "\n",
        "    # Non-linearity\n",
        "    self.non_linearity = nn.GELU()\n",
        "\n",
        "    # Up-projection: from adapter_dim back to model dimension (in_dim)\n",
        "    self.up = nn.Linear(adapter_dim, in_dim)\n",
        "\n",
        "    # Initialize to near-zero to start\n",
        "    nn.init.normal_(self.up.weight, std=1e-4)\n",
        "    nn.init.zeros_(self.up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # The adapter output is added to the input (residual connection)\n",
        "    return x + self.up(self.non_linearity(self.down(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NX4_0Ss09k6"
      },
      "outputs": [],
      "source": [
        "def inject_domain_adapter(mask_decoder, adapter_dim=64):\n",
        "    \"\"\"\n",
        "    Inject domain adapters into SAM's mask decoder MLP blocks.\n",
        "    Properly registers each adapter as a submodule (tracked by .to(device)).\n",
        "    \"\"\"\n",
        "    adapter_idx = 0\n",
        "\n",
        "    modules = list(mask_decoder.named_modules())\n",
        "\n",
        "    for name, module in modules:\n",
        "        if isinstance(module, nn.Linear) and module.out_features == module.in_features:\n",
        "            in_dim = module.out_features\n",
        "            adapter = DecoderAdapter(in_dim, adapter_dim)\n",
        "\n",
        "            # Register adapter properly as a submodule\n",
        "            adapter_name = f\"domain_adapter_{adapter_idx}\"\n",
        "            setattr(mask_decoder, adapter_name, adapter)\n",
        "            adapter_idx += 1\n",
        "\n",
        "            # Wrap original forward\n",
        "            old_forward = module.forward\n",
        "\n",
        "            def new_forward(x, old_forward=old_forward, adapter=getattr(mask_decoder, adapter_name)):\n",
        "                return adapter(old_forward(x))\n",
        "\n",
        "            module.forward = new_forward\n",
        "\n",
        "    return mask_decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojq4LcHhzpvf"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = \"vit_t\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxUETmmAiOSk"
      },
      "source": [
        "**Note:** Please replace your own model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9yZ-F1U0xHM"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/content/drive/MyDrive/FYP/Model_Training/SAM/best_finetunedMobileSAM_DA.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9jUemf50zlm",
        "outputId": "5eca4c2b-4a93-4ac3-bb7e-f36b19f4e77f"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "sam_checkpoint = \"./weights/mobile_sam.pt\"\n",
        "model_type = \"vit_t\" # MobileSAM\n",
        "\n",
        "mobile_sam = sam_model_registry[MODEL_TYPE](checkpoint=sam_checkpoint)\n",
        "mobile_sam.mask_decoder = inject_domain_adapter(mobile_sam.mask_decoder, adapter_dim=64)\n",
        "\n",
        "mobile_sam.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRw8F4nA4_ic"
      },
      "outputs": [],
      "source": [
        "TRAINING_IMG_SIZE = 1024\n",
        "SAM_EMBEDDING_SIZE = 1024\n",
        "\n",
        "class MobileSAMFineTuner(nn.Module):\n",
        "    def __init__(self, sam_model, train_img_size, sam_emb_size):\n",
        "        super().__init__()\n",
        "        self.sam = sam_model\n",
        "        self.train_img_size = train_img_size\n",
        "        self.sam_emb_size = sam_emb_size\n",
        "        self.scale_factor = sam_emb_size / train_img_size\n",
        "\n",
        "        # Freeze image encoder (keep prompt + mask decoder trainable)\n",
        "        for name, param in self.sam.named_parameters():\n",
        "            if name.startswith('image_encoder'):\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, images: torch.Tensor, batch_bboxes: list[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: Tensor [B, 3, H, W]\n",
        "            batch_bboxes: list of length B, each element is a Tensor [N_i, 4]\n",
        "                          where N_i = number of boxes for image i\n",
        "        Returns:\n",
        "            all_masks: list of Tensors, each of shape [N_i, 1, H, W]\n",
        "            all_iou_preds: list of Tensors, each of shape [N_i, 1]\n",
        "        \"\"\"\n",
        "        B, C, H, W = images.shape\n",
        "\n",
        "        # --- Preprocess images (rescale + normalize) ---\n",
        "        preprocessed_images = []\n",
        "        for i in range(B):\n",
        "            preprocessed_img = self.sam.preprocess(images[i])\n",
        "            preprocessed_images.append(preprocessed_img)\n",
        "        input_images = torch.stack(preprocessed_images, dim=0)  # [B, 3, 1024, 1024]\n",
        "\n",
        "        # --- Compute frozen image embeddings ---\n",
        "        with torch.no_grad():\n",
        "            image_embeddings = self.sam.image_encoder(input_images)  # [B, 1024, 64, 64]\n",
        "\n",
        "        all_masks = []\n",
        "        all_iou_preds = []\n",
        "\n",
        "        # --- For each image, process all bounding boxes ---\n",
        "        for i in range(B):\n",
        "            image_embedding_i = image_embeddings[i].unsqueeze(0)  # [1, 1024, 64, 64]\n",
        "            boxes_i = batch_bboxes[i]  # [N_i, 4]\n",
        "\n",
        "            # Scale bounding boxes to match SAM embedding space\n",
        "            scaled_boxes_i = boxes_i * self.scale_factor  # [N_i, 4]\n",
        "\n",
        "            # Encode multiple box prompts at once\n",
        "            sparse_embeddings_i, dense_embeddings_i = self.sam.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=scaled_boxes_i,\n",
        "                masks=None,\n",
        "            )\n",
        "\n",
        "            # Decode all masks for this image\n",
        "            low_res_masks_i, iou_predictions_i = self.sam.mask_decoder(\n",
        "                image_embeddings=image_embedding_i,  # [1, 1024, 64, 64]\n",
        "                image_pe=self.sam.prompt_encoder.get_dense_pe(),\n",
        "                sparse_prompt_embeddings=sparse_embeddings_i,  # [N_i, C]\n",
        "                dense_prompt_embeddings=dense_embeddings_i,    # [1, C, H, W]\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "            # Upsample predicted masks to full image size\n",
        "            upsampled_masks_i = F.interpolate(\n",
        "                low_res_masks_i,\n",
        "                size=(self.train_img_size, self.train_img_size),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "\n",
        "            all_masks.append(upsampled_masks_i)      # [N_i, 1, H, W]\n",
        "            all_iou_preds.append(iou_predictions_i)  # [N_i, 1]\n",
        "\n",
        "        return all_masks, all_iou_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ijx_NX-5EeU",
        "outputId": "8a647113-1863-4d63-afc9-9146b424a41c"
      },
      "outputs": [],
      "source": [
        "finetuner = MobileSAMFineTuner(sam_model=mobile_sam, train_img_size=1024, sam_emb_size=1024)\n",
        "finetuner.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUcLzIqV1NHK",
        "outputId": "bc0642f7-c2e4-43e1-fd9d-fedee0ec6de8"
      },
      "outputs": [],
      "source": [
        "# --- 4. Load the saved model weights ---\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "\n",
        "# --- 5. Load the weights into the model ---\n",
        "missing, unexpected = finetuner.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "print(\"✅ Model loaded successfully.\")\n",
        "print(\"Missing keys:\", missing)\n",
        "print(\"Unexpected keys:\", unexpected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llkO3tuS1VEV",
        "outputId": "b68da319-64e1-45eb-d095-435431f26b44"
      },
      "outputs": [],
      "source": [
        "finetuner.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K084kS88kXGE"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WgRFiw9tjc0"
      },
      "outputs": [],
      "source": [
        "def denormalize_image(image_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor image and converts it to a displayable format (HWC, uint8).\n",
        "\n",
        "    Args:\n",
        "        image_tensor (torch.Tensor): A normalized image tensor (C, H, W).\n",
        "        mean (list or tuple): The mean values used for normalization.\n",
        "        std (list or tuple): The standard deviation values used for normalization.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A denormalized NumPy array in (H, W, C) format with uint8 data type.\n",
        "    \"\"\"\n",
        "    mean = np.array(mean).reshape(1, 1, 3)\n",
        "    std = np.array(std).reshape(1, 1, 3)\n",
        "\n",
        "    # Transpose from (C, H, W) to (H, W, C)\n",
        "    img_np = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Denormalize\n",
        "    img_np = (img_np * std) + mean\n",
        "\n",
        "    # Clip and convert to uint8\n",
        "    img_np = np.clip(img_np, 0, 1) * 255\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    return img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f09t1spw7V8X",
        "outputId": "f763e668-5818-4a49-b496-7ff4a70beac0"
      },
      "outputs": [],
      "source": [
        "# Evaluate on each test set\n",
        "for test_set_name, test_set in test_sets.items():\n",
        "\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "    # ⚠️ Use batch_size=1 for promptable models (different number of boxes per image)\n",
        "\n",
        "    total_tp = total_fp = total_fn = total_tn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch[\"image\"].to(device).float()        # [1, 3, H, W]\n",
        "            masks_gt = batch[\"mask\"].to(device).float()       # [1, 1, H, W]\n",
        "            bboxes = [batch[\"bboxes\"][0].to(device).float()]  # list([N_i, 4])\n",
        "            image_name = batch[\"image_name\"][0]\n",
        "\n",
        "            # Skip if no bounding boxes are detected for this image\n",
        "            if bboxes[0].numel() == 0:\n",
        "                print(f\"Skipping {image_name} from {test_set_name}: No bounding boxes detected.\")\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            pred_masks_list, iou_preds_list = finetuner(images, bboxes)\n",
        "\n",
        "            # Each pred_masks_list[i] has shape [N_i, 1, H, W]\n",
        "            pred_masks = pred_masks_list[0]                   # [N_i, 1, H, W]\n",
        "            mask_gt = masks_gt.squeeze(0)                     # [1, H, W]\n",
        "\n",
        "            # Combine all predicted masks into one binary mask\n",
        "            combined_pred_mask = (torch.sigmoid(pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "            combined_pred_mask = (combined_pred_mask > 0).float()  # Union of all object masks\n",
        "\n",
        "            # Compute confusion matrix components for this image\n",
        "            tp, fp, fn, tn = get_confusion_matrix_components(mask_gt.unsqueeze(0), combined_pred_mask.unsqueeze(0))\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "            total_tn += tn\n",
        "\n",
        "    # --- Final metrics for the test set ---\n",
        "    avg_iou, avg_dice, avg_recall, avg_precision, avg_accuracy = calculate_final_metrics(\n",
        "        total_tp, total_fp, total_fn, total_tn\n",
        "    )\n",
        "\n",
        "    print(f\"\\nMetrics for {test_set_name}:\")\n",
        "    print(f\"  IoU: {avg_iou:.4f}\")\n",
        "    print(f\"  Dice: {avg_dice:.4f}\")\n",
        "    print(f\"  Recall: {avg_recall:.4f}\")\n",
        "    print(f\"  Precision: {avg_precision:.4f}\")\n",
        "    print(f\"  Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    # --- Visualization Section ---\n",
        "    # Get a sample batch for visualization (ensure it has bounding boxes)\n",
        "    vis_batch = None\n",
        "    for batch in DataLoader(test_set, batch_size=1, shuffle=True):\n",
        "        if batch[\"bboxes\"][0].numel() > 0:\n",
        "            vis_batch = batch\n",
        "            break\n",
        "\n",
        "    if vis_batch is None:\n",
        "        print(f\"Could not find a sample with bounding boxes in {test_set_name} for visualization.\")\n",
        "        continue\n",
        "\n",
        "    vis_images = vis_batch[\"image\"].to(device).float()\n",
        "    vis_masks_gt = vis_batch[\"mask\"].to(device).float()\n",
        "    vis_bboxes = [vis_batch[\"bboxes\"][0].to(device).float()]\n",
        "    vis_image_name = vis_batch[\"image_name\"]\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vis_pred_masks_list, _ = finetuner(vis_images, vis_bboxes)\n",
        "        vis_pred_masks = vis_pred_masks_list[0]  # [N, 1, H, W]\n",
        "        vis_pred_combined = (torch.sigmoid(vis_pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "        vis_pred_combined = (vis_pred_combined > 0).float()\n",
        "\n",
        "    # --- Display visualization ---\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
        "    fig.suptitle(f\"Sample Predictions - {test_set_name}\", fontsize=14)\n",
        "\n",
        "    # Original image\n",
        "    original_img = denormalize_image(vis_images[0], IMAGENET_MEAN, IMAGENET_STD)\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Ground Truth\n",
        "    gt_mask = vis_masks_gt[0].squeeze().cpu().numpy()\n",
        "    axes[1].imshow(original_img)\n",
        "    axes[1].imshow(gt_mask, cmap=\"Reds\", alpha=0.5)\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Predicted Mask\n",
        "    pred_mask = vis_pred_combined[0].squeeze().cpu().numpy()\n",
        "    axes[2].imshow(original_img)\n",
        "    axes[2].imshow(pred_mask, cmap=\"Blues\", alpha=0.5)\n",
        "    axes[2].set_title(\"Predicted Mask\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
