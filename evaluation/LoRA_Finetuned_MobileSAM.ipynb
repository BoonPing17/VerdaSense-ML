{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmmMNJBSfTOc",
        "outputId": "bf6adc17-c467-4cbd-f59a-23dceca9d4d2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyAfJjWfoXf",
        "outputId": "98051c03-fbf2-43ba-c594-c09f91dcd9f5"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!mkdir -p weights\n",
        "!wget -nc https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK2RiNPPf1bZ",
        "outputId": "abc11d54-7742-4a32-d843-1300100911a8"
      },
      "outputs": [],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHXFsJlff3a7",
        "outputId": "f094c5a4-1291-4dfe-bb6a-a8dd81476d05"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from mobile_sam import sam_model_registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMum6w0gHYO"
      },
      "source": [
        "## Evaluation Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNREbkagD2g"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya4ccR0ugSqp"
      },
      "outputs": [],
      "source": [
        "class EvaluationPromptableSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_bounding_boxes_from_mask(self, mask, padding_factor=0.1, min_area_threshold=5):\n",
        "        \"\"\"Get one or multiple bounding boxes from binary mask.\"\"\"\n",
        "        _, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "        boxes = []\n",
        "        for i in range(1, num_labels):\n",
        "            x, y, w, h, area = (\n",
        "                stats[i, cv2.CC_STAT_LEFT],\n",
        "                stats[i, cv2.CC_STAT_TOP],\n",
        "                stats[i, cv2.CC_STAT_WIDTH],\n",
        "                stats[i, cv2.CC_STAT_HEIGHT],\n",
        "                stats[i, cv2.CC_STAT_AREA],\n",
        "            )\n",
        "            if area < min_area_threshold:\n",
        "                continue\n",
        "\n",
        "            pad = int(max(w, h) * padding_factor)\n",
        "            x_min, y_min = max(0, x - pad), max(0, y - pad)\n",
        "            x_max, y_max = min(mask.shape[1], x + w + pad), min(mask.shape[0], y + h + pad)\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = self.masks[idx]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Load image and mask\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        original_H, original_W = mask.shape\n",
        "        boxes = self.get_bounding_boxes_from_mask(mask)\n",
        "\n",
        "        # Resize\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "        if mask.shape[:2] != self.img_size:\n",
        "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Image normalization\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        mask_tensor = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n",
        "\n",
        "        # Return all boxes, not just one (SAM can handle multiple)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"mask\": mask_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86gNenPds1dG"
      },
      "source": [
        "**Note:** Replace your own paths here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2B_su7kgTwz"
      },
      "outputs": [],
      "source": [
        "test_sets_base_path = \"/content/drive/MyDrive/FYP/Datasets/\"\n",
        "test_set_1_images_path = os.path.join(test_sets_base_path, \"FUSeg/test/images\")\n",
        "test_set_1_masks_path = os.path.join(test_sets_base_path, \"FUSeg/test/labels\")\n",
        "\n",
        "test_set_2_images_path = os.path.join(test_sets_base_path, \"DFUC2022/test/images\")\n",
        "test_set_2_masks_path = os.path.join(test_sets_base_path, \"DFUC2022/test/masks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMzMimNQgU_G"
      },
      "outputs": [],
      "source": [
        "test_sets = {\n",
        "    \"test_set_FUSeg\": EvaluationPromptableSegmentationDataset(test_set_1_images_path, test_set_1_masks_path),\n",
        "    \"test_set_DFUC2022\": EvaluationPromptableSegmentationDataset(test_set_2_images_path, test_set_2_masks_path),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT6WdzeJgZCF"
      },
      "source": [
        "## Helper function to get evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb0vxBDtgckr"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix_components(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the confusion matrix components (TP, FP, FN) for a batch.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground truth masks, a tensor of 0s and 1s.\n",
        "        y_pred (torch.Tensor): Predicted masks from the model, a tensor of continuous values.\n",
        "        threshold (float): The binarization threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing True Positives, False Positives, False Negatives, and True Negatives.\n",
        "    \"\"\"\n",
        "    # Flatten tensors for easier calculation\n",
        "    y_true_flat = y_true.view(-1)\n",
        "    y_pred_flat = y_pred.view(-1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    true_positives = ((y_pred_flat == 1) & (y_true_flat == 1)).sum().item()\n",
        "    false_positives = ((y_pred_flat == 1) & (y_true_flat == 0)).sum().item()\n",
        "    false_negatives = ((y_pred_flat == 0) & (y_true_flat == 1)).sum().item()\n",
        "    true_negatives = ((y_pred_flat == 0) & (y_true_flat == 0)).sum().item()\n",
        "\n",
        "    return true_positives, false_positives, false_negatives, true_negatives\n",
        "\n",
        "# Final metrics calculation function\n",
        "def calculate_final_metrics(tp, fp, fn, tn, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Calculates final metrics from accumulated confusion matrix components.\n",
        "    \"\"\"\n",
        "    # IoU\n",
        "    intersection = tp\n",
        "    union = tp + fp + fn\n",
        "    iou = intersection / (union + smooth)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * tp) / (2 * tp + fp + fn + smooth)\n",
        "\n",
        "    # Recall (Sensitivity)\n",
        "    recall = tp / (tp + fn + smooth)\n",
        "\n",
        "    # Precision (Positive Predictive Value)\n",
        "    precision = tp / (tp + fp + smooth)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + smooth)\n",
        "\n",
        "    return iou, dice, recall, precision, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtqB3qMgeXP"
      },
      "source": [
        "## MobileSAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZJZNZ4ShSKl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0RjsKhGgy7N"
      },
      "outputs": [],
      "source": [
        "# Load the MobileSAM model\n",
        "model_type = \"vit_t\"\n",
        "checkpoint_path = \"./weights/mobile_sam.pt\"\n",
        "\n",
        "mobile_sam = sam_model_registry[model_type](checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjvw1OZslpW"
      },
      "source": [
        "**Note:** Replace your own paths here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H5bHHFTB_Fv",
        "outputId": "e169cf3e-4e8b-4fac-b6c3-b5a506c141c2"
      },
      "outputs": [],
      "source": [
        "# Load trained mask decoder\n",
        "decoder_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/mask_decoder.pth\"\n",
        "\n",
        "# Capture the loading message\n",
        "msg = mobile_sam.mask_decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
        "\n",
        "# Check for issues\n",
        "if len(msg.missing_keys) == 0 and len(msg.unexpected_keys) == 0:\n",
        "    print(\"✅ Mask Decoder: All weights loaded successfully with no mismatches.\")\n",
        "else:\n",
        "    print(\"⚠️ Mask Decoder Load Warning:\")\n",
        "    print(f\"   Missing Keys: {msg.missing_keys}\")\n",
        "    print(f\"   Unexpected Keys: {msg.unexpected_keys}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94MjrjrCCK-",
        "outputId": "36061089-70a4-4bd6-828c-fd8f8571daea"
      },
      "outputs": [],
      "source": [
        "# Load LoRA into the image encoder\n",
        "lora_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/lora_image_encoder\"\n",
        "mobile_sam.image_encoder = PeftModel.from_pretrained(mobile_sam.image_encoder, lora_path)\n",
        "\n",
        "# Verification Steps:\n",
        "# 1. Check if an adapter is active\n",
        "active_adapters = mobile_sam.image_encoder.active_adapters\n",
        "print(f\"✅ Active LoRA Adapters: {active_adapters}\")\n",
        "\n",
        "# 2. Check for missing keys (PEFT models often warn during .from_pretrained)\n",
        "# If you want to be 100% sure, check if the lora layers exist in the modules\n",
        "has_lora = any(\"lora_\" in name for name, _ in mobile_sam.image_encoder.named_modules())\n",
        "if has_lora:\n",
        "    print(\"✅ LoRA layers detected in the Image Encoder.\")\n",
        "else:\n",
        "    print(\"❌ ERROR: No LoRA layers found. The adapter was not applied correctly.\")\n",
        "\n",
        "# 3. Print trainable parameters (should be 0 for inference, but confirms structure)\n",
        "mobile_sam.image_encoder.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYBkcvk_hJtH",
        "outputId": "62a0a51d-2bff-4bf2-c341-392bc380f0d9"
      },
      "outputs": [],
      "source": [
        "mobile_sam.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r02lL1z0hLnz"
      },
      "source": [
        "## Finetuner Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkKtxmhzhNsF"
      },
      "outputs": [],
      "source": [
        "class MobileSAMFineTuner(nn.Module):\n",
        "    def __init__(self, sam_model):\n",
        "        super().__init__()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def forward(self, images: torch.Tensor, bboxes: list):\n",
        "        # images: [B, 3, 1024, 1024]\n",
        "        # bboxes: List of tensors, where bboxes[i] is [N_boxes, 4]\n",
        "\n",
        "        _, _, H, W = images.shape\n",
        "\n",
        "        # 1. Compute Image Embeddings (Run once per image)\n",
        "        image_embeddings = self.sam.image_encoder(images)\n",
        "        dense_pe = self.sam.prompt_encoder.get_dense_pe()\n",
        "\n",
        "        # Prepare lists to match the \"Previous Wrapper\" return format\n",
        "        final_masks_list = []\n",
        "        iou_preds_list = []\n",
        "\n",
        "        B = len(bboxes)\n",
        "\n",
        "        for i in range(B):\n",
        "            curr_box = bboxes[i] # Shape [N, 4]\n",
        "\n",
        "            # Safety check for images with no boxes\n",
        "            if curr_box.shape[0] == 0:\n",
        "                 # Return empty tensors so the list index stays aligned\n",
        "                 final_masks_list.append(torch.zeros(0, 1, H, W, device=images.device))\n",
        "                 iou_preds_list.append(torch.zeros(0, 1, device=images.device))\n",
        "                 continue\n",
        "\n",
        "            curr_embedding = image_embeddings[i].unsqueeze(0)\n",
        "\n",
        "            # Prompt encoder (Handles N boxes)\n",
        "            sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=curr_box,\n",
        "                masks=None,\n",
        "            )\n",
        "\n",
        "            # Mask decoder\n",
        "            low_res_masks, iou_predictions = self.sam.mask_decoder(\n",
        "                image_embeddings=curr_embedding,\n",
        "                image_pe=dense_pe,\n",
        "                sparse_prompt_embeddings=sparse_embeddings,\n",
        "                dense_prompt_embeddings=dense_embeddings,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "            # low_res_masks shape: [N, 1, 256, 256]\n",
        "\n",
        "            # Upsample NOW (per image) instead of stacking first\n",
        "            upsampled_masks = F.interpolate(\n",
        "                low_res_masks,\n",
        "                size=(H, W),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            # upsampled_masks shape: [N, 1, 1024, 1024]\n",
        "\n",
        "            final_masks_list.append(upsampled_masks)\n",
        "            iou_preds_list.append(iou_predictions)\n",
        "\n",
        "        # Return LISTS, not stacked tensors.\n",
        "        # The evaluation loop will access [0] to get the tensor for the first image.\n",
        "        return final_masks_list, iou_preds_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duu8C8-_hPq8"
      },
      "outputs": [],
      "source": [
        "finetuner = MobileSAMFineTuner(sam_model=mobile_sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIRblBl3hfEr",
        "outputId": "33c25ec6-c588-44b2-9801-4c54f11836f1"
      },
      "outputs": [],
      "source": [
        "finetuner.to(device)\n",
        "finetuner.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-ua7cglhgxf"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8PqpzhNhh7g"
      },
      "outputs": [],
      "source": [
        "def denormalize_image(image_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor image and converts it to a displayable format (HWC, uint8).\n",
        "\n",
        "    Args:\n",
        "        image_tensor (torch.Tensor): A normalized image tensor (C, H, W).\n",
        "        mean (list or tuple): The mean values used for normalization.\n",
        "        std (list or tuple): The standard deviation values used for normalization.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A denormalized NumPy array in (H, W, C) format with uint8 data type.\n",
        "    \"\"\"\n",
        "    mean = np.array(mean).reshape(1, 1, 3)\n",
        "    std = np.array(std).reshape(1, 1, 3)\n",
        "\n",
        "    # Transpose from (C, H, W) to (H, W, C)\n",
        "    img_np = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Denormalize\n",
        "    img_np = (img_np * std) + mean\n",
        "\n",
        "    # Clip and convert to uint8\n",
        "    img_np = np.clip(img_np, 0, 1) * 255\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    return img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntnuHWCahjqG"
      },
      "outputs": [],
      "source": [
        "def overlay_bbox_on_image(image_tensor, bboxes_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Convert normalized tensor to RGB image and draw all bboxes.\n",
        "    image_tensor: [3, H, W] torch.Tensor\n",
        "    bboxes_tensor: [N, 4] torch.Tensor (x_min, y_min, x_max, y_max)\n",
        "    \"\"\"\n",
        "    # Denormalize\n",
        "    img = denormalize_image(image_tensor, mean, std)  # returns HWC, values [0,1]\n",
        "    img = img.copy()\n",
        "\n",
        "    # Handle case where bboxes might be a list (extract first element) or single tensor\n",
        "    if isinstance(bboxes_tensor, list):\n",
        "        bboxes_tensor = bboxes_tensor[0]\n",
        "\n",
        "    # Convert to numpy\n",
        "    bboxes = bboxes_tensor.cpu().numpy().astype(int)\n",
        "\n",
        "    # Loop through each bounding box and draw it\n",
        "    for box in bboxes:\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=(255, 0, 0), thickness=2)\n",
        "\n",
        "    return img  # HWC, RGB uint8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jhNxI6LbhtIF",
        "outputId": "f83c8697-80fd-49f3-d0c4-c11495b00fd4"
      },
      "outputs": [],
      "source": [
        "# Evaluate on each test set\n",
        "for test_set_name, test_set in test_sets.items():\n",
        "\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
        "    # ⚠️ Use batch_size=1 for promptable models (different number of boxes per image)\n",
        "\n",
        "    total_tp = total_fp = total_fn = total_tn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch[\"image\"].to(device).float()        # [1, 3, H, W]\n",
        "            masks_gt = batch[\"mask\"].to(device).float()       # [1, 1, H, W]\n",
        "            bboxes = [batch[\"bboxes\"][0].to(device).float()]  # list([N_i, 4])\n",
        "            image_name = batch[\"image_name\"][0]\n",
        "\n",
        "            # Skip if no bounding boxes are detected for this image\n",
        "            if bboxes[0].numel() == 0:\n",
        "                print(f\"Skipping {image_name} from {test_set_name}: No bounding boxes detected.\")\n",
        "                continue\n",
        "\n",
        "            # Forward pass\n",
        "            pred_masks_list, iou_preds_list = finetuner(images, bboxes)\n",
        "\n",
        "            # Each pred_masks_list[i] has shape [N_i, 1, H, W]\n",
        "            pred_masks = pred_masks_list[0]                   # [N_i, 1, H, W]\n",
        "            mask_gt = masks_gt.squeeze(0)                     # [1, H, W]\n",
        "\n",
        "            # Combine all predicted masks into one binary mask\n",
        "            combined_pred_mask = (torch.sigmoid(pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "            combined_pred_mask = (combined_pred_mask > 0).float()  # Union of all object masks\n",
        "\n",
        "            # Compute confusion matrix components for this image\n",
        "            tp, fp, fn, tn = get_confusion_matrix_components(mask_gt.unsqueeze(0), combined_pred_mask.unsqueeze(0))\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "            total_tn += tn\n",
        "\n",
        "    # --- Final metrics for the test set ---\n",
        "    avg_iou, avg_dice, avg_recall, avg_precision, avg_accuracy = calculate_final_metrics(\n",
        "        total_tp, total_fp, total_fn, total_tn\n",
        "    )\n",
        "\n",
        "    print(f\"\\nMetrics for {test_set_name}:\")\n",
        "    print(f\"  IoU: {avg_iou:.4f}\")\n",
        "    print(f\"  Dice: {avg_dice:.4f}\")\n",
        "    print(f\"  Recall: {avg_recall:.4f}\")\n",
        "    print(f\"  Precision: {avg_precision:.4f}\")\n",
        "    print(f\"  Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    # --- Visualization Section ---\n",
        "    # Get a sample batch for visualization (ensure it has bounding boxes)\n",
        "    vis_batch = None\n",
        "    for batch in DataLoader(test_set, batch_size=1, shuffle=True):\n",
        "        if batch[\"bboxes\"][0].numel() > 0:\n",
        "            vis_batch = batch\n",
        "            break\n",
        "\n",
        "    if vis_batch is None:\n",
        "        print(f\"Could not find a sample with bounding boxes in {test_set_name} for visualization.\")\n",
        "        continue\n",
        "\n",
        "    vis_images = vis_batch[\"image\"].to(device).float()\n",
        "    vis_masks_gt = vis_batch[\"mask\"].to(device).float()\n",
        "    vis_bboxes = [vis_batch[\"bboxes\"][0].to(device).float()]\n",
        "    vis_image_name = vis_batch[\"image_name\"]\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vis_pred_masks_list, _ = finetuner(vis_images, vis_bboxes)\n",
        "        vis_pred_masks = vis_pred_masks_list[0]  # [N, 1, H, W]\n",
        "        vis_pred_combined = (torch.sigmoid(vis_pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "        vis_pred_combined = (vis_pred_combined > 0).float()\n",
        "\n",
        "    # --- Display visualization ---\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
        "    fig.suptitle(f\"Sample Predictions - {test_set_name}\", fontsize=14)\n",
        "\n",
        "    # Original image\n",
        "    original_img = denormalize_image(vis_images[0], IMAGENET_MEAN, IMAGENET_STD)\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Ground Truth\n",
        "    gt_mask = vis_masks_gt[0].squeeze().cpu().numpy()\n",
        "    axes[1].imshow(original_img)\n",
        "    axes[1].imshow(gt_mask, cmap=\"Reds\", alpha=0.5)\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Predicted Mask\n",
        "    pred_mask = vis_pred_combined[0].squeeze().cpu().numpy()\n",
        "    axes[2].imshow(original_img)\n",
        "    axes[2].imshow(pred_mask, cmap=\"Blues\", alpha=0.5)\n",
        "    axes[2].set_title(\"Predicted Mask\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB1totL7li0B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
