{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlcmsYP8iukI",
        "outputId": "0e30a1a3-48ed-4fee-f1fe-976a502b1d0c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EXLKQiPfv4K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP9tSpzAibUR"
      },
      "outputs": [],
      "source": [
        "# Standard ImageNet Normalization\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rQnK1IWhXPC"
      },
      "outputs": [],
      "source": [
        "class EvaluationSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = self.masks[idx]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Albumentations expects a NumPy array with uint8 data type\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Manual steps for resize, standardization, and conversion\n",
        "        # 1. Resize if necessary\n",
        "        if image.shape[0] != self.img_size[0] or image.shape[1] != self.img_size[1]:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "        if mask.shape[0] != self.img_size[0] or mask.shape[1] != self.img_size[1]:\n",
        "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # 2. Convert image to float and standardize\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "\n",
        "        # 3. Convert image to torch tensor and permute (HWC -> CHW)\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        # 4. Convert mask to float and add a channel dimension\n",
        "        mask = (mask > 0).astype(\"float32\") # Convert to binary (0.0 or 1.0)\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0) # HW -> 1HW\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMSt6wXfgKhP"
      },
      "source": [
        "**Note:** Replace your own test paths here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cOlJjtWehXRV"
      },
      "outputs": [],
      "source": [
        "test_sets_base_path = \"/content/drive/MyDrive/FYP/Datasets/\"\n",
        "test_set_1_images_path = os.path.join(test_sets_base_path, \"FUSeg/test/images\")\n",
        "test_set_1_masks_path = os.path.join(test_sets_base_path, \"FUSeg/test/labels\")\n",
        "\n",
        "test_set_2_images_path = os.path.join(test_sets_base_path, \"DFUC2022/test/images\")\n",
        "test_set_2_masks_path = os.path.join(test_sets_base_path, \"DFUC2022/test/masks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0uCUWbNhXTd"
      },
      "outputs": [],
      "source": [
        "test_sets = {\n",
        "    \"test_set_fuseg\": EvaluationSegmentationDataset(test_set_1_images_path, test_set_1_masks_path),\n",
        "    \"test_set_dfuc2022\": EvaluationSegmentationDataset(test_set_2_images_path, test_set_2_masks_path),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq96e7eoq3Y"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB03lc9Losyq"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix_components(y_true, y_pred, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates the confusion matrix components (TP, FP, FN) for a batch.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground truth masks, a tensor of 0s and 1s.\n",
        "        y_pred (torch.Tensor): Predicted masks from the model, a tensor of continuous values.\n",
        "        threshold (float): The binarization threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing True Positives, False Positives, False Negatives, and True Negatives.\n",
        "    \"\"\"\n",
        "    # Binarize predictions using the specified threshold\n",
        "    y_pred = (y_pred > threshold).float()\n",
        "\n",
        "    # Flatten tensors for easier calculation\n",
        "    y_true_flat = y_true.view(-1)\n",
        "    y_pred_flat = y_pred.view(-1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    true_positives = ((y_pred_flat == 1) & (y_true_flat == 1)).sum().item()\n",
        "    false_positives = ((y_pred_flat == 1) & (y_true_flat == 0)).sum().item()\n",
        "    false_negatives = ((y_pred_flat == 0) & (y_true_flat == 1)).sum().item()\n",
        "    true_negatives = ((y_pred_flat == 0) & (y_true_flat == 0)).sum().item()\n",
        "\n",
        "    return true_positives, false_positives, false_negatives, true_negatives\n",
        "\n",
        "# Final metrics calculation function\n",
        "def calculate_final_metrics(tp, fp, fn, tn, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Calculates final metrics from accumulated confusion matrix components.\n",
        "    \"\"\"\n",
        "    # IoU\n",
        "    intersection = tp\n",
        "    union = tp + fp + fn\n",
        "    iou = intersection / (union + smooth)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * tp) / (2 * tp + fp + fn + smooth)\n",
        "\n",
        "    # Recall (Sensitivity)\n",
        "    recall = tp / (tp + fn + smooth)\n",
        "\n",
        "    # Precision (Positive Predictive Value)\n",
        "    precision = tp / (tp + fp + smooth)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + smooth)\n",
        "\n",
        "    return iou, dice, recall, precision, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKnL18_pka56"
      },
      "source": [
        "## DeepLabv3+ with MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo6GVAgWnKPj"
      },
      "outputs": [],
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Depthwise Separable Convolution, which is a depthwise convolution\n",
        "    followed by a pointwise (1x1) convolution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, bias=False):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "\n",
        "        # Calculate padding to keep spatial size same: p = (d * (k-1)) / 2\n",
        "        padding = dilation\n",
        "\n",
        "        # Depthwise convolution: Applies a separate filter to each input channel\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n",
        "        self.bn_depth = nn.BatchNorm2d(in_channels)\n",
        "        self.relu_depth = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Pointwise convolution: A 1x1 convolution to mix the channels\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=bias)\n",
        "        self.bn_point = nn.BatchNorm2d(out_channels)\n",
        "        self.relu_point = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.bn_depth(x)\n",
        "        x = self.relu_depth(x)\n",
        "\n",
        "        x = self.pointwise(x)\n",
        "        x = self.bn_point(x)\n",
        "        x = self.relu_point(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twgshZodnKR8"
      },
      "outputs": [],
      "source": [
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        # 1x1 convolution branch (This is always a standard 1x1 convolution)\n",
        "        self.conv1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Atrous separable convolution with rate=6\n",
        "        self.atrous_block6 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=6)\n",
        "\n",
        "        # Atrous separable convolution with rate=12\n",
        "        self.atrous_block12 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=12)\n",
        "\n",
        "        # Atrous separable convolution with rate=18\n",
        "        self.atrous_block18 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=18)\n",
        "\n",
        "        # Global Average Pooling branch\n",
        "        self.global_avg_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Final 1x1 convolution to fuse all 5 branches\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(out_channels * 5, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "\n",
        "        x1 = self.conv1x1(x)\n",
        "        x2 = self.atrous_block6(x)\n",
        "        x3 = self.atrous_block12(x)\n",
        "        x4 = self.atrous_block18(x)\n",
        "        x5 = self.global_avg_pool(x)\n",
        "\n",
        "        x5 = F.interpolate(x5, size=size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDIzvgLgnKUS"
      },
      "outputs": [],
      "source": [
        "class DeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DeepLabV3Plus, self).__init__()\n",
        "        backbone = models.mobilenet_v2(weights=\"DEFAULT\")\n",
        "        self.backbone = backbone.features  # Get all layers except classifier\n",
        "\n",
        "        # Modify MobileNetV2 for Output Stride 16\n",
        "        # Change stride of the 14th block (bottleneck)\n",
        "        self.backbone[14].conv[1][0].stride = (1, 1)\n",
        "\n",
        "        # Then all subsequent layers must use dilation=2 to maintain receptive field\n",
        "        for i in range (14, 19):\n",
        "          for m in self.backbone[i].modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "              # Only apply to 3x3 depthwise convs\n",
        "              if m.kernel_size == (3, 3):\n",
        "                m.dilation = (2, 2)\n",
        "                m.padding = (2, 2)\n",
        "\n",
        "        # Low-level features come from early layer (for decoder)\n",
        "        self.low_level_idx = 3\n",
        "        self.low_level_channels = 24\n",
        "\n",
        "        # ASPP expects 1280 channels from the last MobileNetV2 layer\n",
        "        self.aspp = ASPP(in_channels=1280, out_channels=256)\n",
        "\n",
        "        # Decoder\n",
        "        self.low_level_project = nn.Sequential(\n",
        "            nn.Conv2d(self.low_level_channels, 48, kernel_size=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      input_size = x.size()[2:]\n",
        "\n",
        "      # Extract low-level and high-level features\n",
        "      low_level_feat = None\n",
        "      feat = x\n",
        "      for i, layer in enumerate(self.backbone):\n",
        "          feat = layer(feat)\n",
        "          if i == self.low_level_idx:\n",
        "              low_level_feat = feat  # Save for decoder\n",
        "\n",
        "      high_level_feat = feat  # Final output of backbone (usually [B, 1280, H/32, W/32])\n",
        "\n",
        "      # ASPP on high-level features\n",
        "      x = self.aspp(high_level_feat)\n",
        "      x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "      # Decoder\n",
        "      low_level = self.low_level_project(low_level_feat)\n",
        "      x = torch.cat([x, low_level], dim=1)\n",
        "      x = self.decoder(x)\n",
        "      x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hrt0xU8nyBY",
        "outputId": "6a1c85a5-0890-4200-d19f-90f1cae3925e"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yQuem6XoGWU",
        "outputId": "f8b569c1-dc1f-4fa2-b602-030dd4d2bdab"
      },
      "outputs": [],
      "source": [
        "model = DeepLabV3Plus(num_classes=1)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxdQnvJoJLn"
      },
      "source": [
        "## Load the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACqyj5GBgTlm"
      },
      "source": [
        "**Note:** Replace your own model checkpoint file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgbP7k6OnKW4"
      },
      "outputs": [],
      "source": [
        "# Define the path to your saved model file\n",
        "model_path = \"/content/drive/MyDrive/FYP/Model_Training/MobileNet/checkpoints/Run_20260117-0531372/best_DeepLabv3PlusModelwithMobileNetV2.pth\"\n",
        "\n",
        "# Load the entire model state dictionary\n",
        "state_dict = torch.load(model_path, map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY60DQkBnrtw",
        "outputId": "3f738123-60af-4786-be9f-bd9f999d5660"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K084kS88kXGE"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WgRFiw9tjc0"
      },
      "outputs": [],
      "source": [
        "def denormalize_image(image_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor image and converts it to a displayable format (HWC, uint8).\n",
        "\n",
        "    Args:\n",
        "        image_tensor (torch.Tensor): A normalized image tensor (C, H, W).\n",
        "        mean (list or tuple): The mean values used for normalization.\n",
        "        std (list or tuple): The standard deviation values used for normalization.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A denormalized NumPy array in (H, W, C) format with uint8 data type.\n",
        "    \"\"\"\n",
        "    mean = np.array(mean).reshape(1, 1, 3)\n",
        "    std = np.array(std).reshape(1, 1, 3)\n",
        "\n",
        "    # Transpose from (C, H, W) to (H, W, C)\n",
        "    img_np = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Denormalize\n",
        "    img_np = (img_np * std) + mean\n",
        "\n",
        "    # Clip and convert to uint8\n",
        "    img_np = np.clip(img_np, 0, 1) * 255\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    return img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MIWP5YNihXX4",
        "outputId": "1b2a1d6f-7d46-479d-aec4-2f2206b9060a"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on each test set\n",
        "for test_set_name, test_set in test_sets.items():\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    total_tp = total_fp = total_fn = total_tn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_test_batch, y_test_batch in test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device).float(), y_test_batch.to(device).float()\n",
        "\n",
        "            predictions_logits = model(X_test_batch)\n",
        "            predictions_probs = torch.sigmoid(predictions_logits)\n",
        "\n",
        "            tp, fp, fn, tn = get_confusion_matrix_components(y_test_batch, predictions_probs, threshold=0.5)\n",
        "\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "            total_tn += tn\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_iou, avg_dice, avg_recall, avg_precision, avg_accuracy = calculate_final_metrics(total_tp, total_fp, total_fn, total_tn)\n",
        "\n",
        "    print(f\"\\nMetrics for {test_set_name}:\")\n",
        "    print(f\"  IoU: {avg_iou:.4f}\")\n",
        "    print(f\"  Dice: {avg_dice:.4f}\")\n",
        "    print(f\"  Recall: {avg_recall:.4f}\")\n",
        "    print(f\"  Precision: {avg_precision:.4f}\")\n",
        "    print(f\"  Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    # --- Visualization (One-by-One Overlay) ---\n",
        "    try:\n",
        "        # Grab a single sample batch for visualization\n",
        "        vis_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
        "        vis_batch = next(iter(vis_loader))\n",
        "        vis_image, vis_label = vis_batch[0].to(device).float(), vis_batch[1].to(device).float()\n",
        "    except StopIteration:\n",
        "        print(f\"Test set {test_set_name} is empty.\")\n",
        "        continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_pred_logits = model(vis_image)\n",
        "        test_pred_mask = (torch.sigmoid(test_pred_logits) > 0.5).float()\n",
        "\n",
        "    # Create the 1x3 display\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle(f\"Sample Prediction - {test_set_name}\", fontsize=14)\n",
        "\n",
        "    # 1. Original image\n",
        "    original_img = denormalize_image(vis_image[0], IMAGENET_MEAN, IMAGENET_STD)\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 2. Ground Truth Overlay (Reds)\n",
        "    gt_mask = vis_label[0].squeeze().cpu().numpy()\n",
        "    axes[1].imshow(original_img)\n",
        "    axes[1].imshow(gt_mask, cmap=\"Reds\", alpha=0.5) # Overlay effect\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 3. Predicted Mask Overlay (Blues)\n",
        "    pred_mask = test_pred_mask[0].squeeze().cpu().numpy()\n",
        "    axes[2].imshow(original_img)\n",
        "    axes[2].imshow(pred_mask, cmap=\"Blues\", alpha=0.5) # Overlay effect\n",
        "    axes[2].set_title(\"Predicted Mask\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
