{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmmMNJBSfTOc",
        "outputId": "81069b83-ab3f-4229-c6e1-9256b2ae8c5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyAfJjWfoXf",
        "outputId": "62d6422b-24f3-4f86-bfe2-f2b2920799b2"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!mkdir -p weights\n",
        "!wget -nc https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK2RiNPPf1bZ",
        "outputId": "ecd27edc-f358-40a2-da4f-92a9a339ccf1"
      },
      "outputs": [],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHXFsJlff3a7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from mobile_sam import sam_model_registry\n",
        "\n",
        "# Tissue Classification\n",
        "from typing import Any\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMum6w0gHYO"
      },
      "source": [
        "## Custom Dataset classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXxJe8MWGQwG"
      },
      "source": [
        "There are two custom dataset classes defined here. Please choose the one which is suitable for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNREbkagD2g"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4__VmEYoFJta"
      },
      "source": [
        "### SegmentationDataset class\n",
        "\n",
        "This class is for the dataset where it contains the ground truth segmentation masks for bounding box(es) extraction. Just need to pass the paths that contain the images and ground truth masks. It will automatically prepare the bounding box(es) coordinates from the ground truth masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya4ccR0ugSqp"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_bounding_boxes_from_mask(self, mask, padding_factor=0.1, min_area_threshold=5):\n",
        "        \"\"\"Get one or multiple bounding boxes from binary mask.\"\"\"\n",
        "        _, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "        boxes = []\n",
        "        for i in range(1, num_labels):\n",
        "            x, y, w, h, area = (\n",
        "                stats[i, cv2.CC_STAT_LEFT],\n",
        "                stats[i, cv2.CC_STAT_TOP],\n",
        "                stats[i, cv2.CC_STAT_WIDTH],\n",
        "                stats[i, cv2.CC_STAT_HEIGHT],\n",
        "                stats[i, cv2.CC_STAT_AREA],\n",
        "            )\n",
        "            if area < min_area_threshold:\n",
        "                continue\n",
        "\n",
        "            pad = int(max(w, h) * padding_factor)\n",
        "            x_min, y_min = max(0, x - pad), max(0, y - pad)\n",
        "            x_max, y_max = min(mask.shape[1], x + w + pad), min(mask.shape[0], y + h + pad)\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = self.masks[idx]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Load image and mask\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        original_H, original_W = mask.shape\n",
        "        boxes = self.get_bounding_boxes_from_mask(mask)\n",
        "\n",
        "        # Resize\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "        if mask.shape[:2] != self.img_size:\n",
        "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Image normalization\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        mask_tensor = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n",
        "\n",
        "        # Return all boxes, not just one (SAM can handle multiple)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"mask\": mask_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6nvBCXQFpLa"
      },
      "source": [
        "**Note:** Please modify the paths below to your own paths that contain the wound images and their ground truth masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgZ-xMUD1BXm"
      },
      "outputs": [],
      "source": [
        "dataset_images_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/images/\"\n",
        "dataset_masks_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/masks/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UES1v_lS1LSj"
      },
      "outputs": [],
      "source": [
        "dataset = SegmentationDataset(dataset_images_path, dataset_masks_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3fvemCDDqd"
      },
      "source": [
        "### RealWorldInferenceDataset class\n",
        "This class is for real world collected dataset that has no ground truth segmentation masks for bounding box(es) extraction. Therefore, we need to manually annotate the bounding box(es) of the wound image in PASCAL format (x1, y1, x2, y2) and record them in a csv file.\n",
        "\n",
        "About the csv file, it must contain two columns: **image_name** and **bbox**. If you have different column names, you can also modify the code below to match the column names in your csv file. Please ensure your image_name is matched and the bounding box is in the correct format: [[x1, y1, x2, y2]]. If there are two bounding boxes, the format is [[x1, y1, x2, y2], [x1, y1, x2, y2]]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-g3k6AzA1IK"
      },
      "outputs": [],
      "source": [
        "class RealWorldInferenceDataset(Dataset):\n",
        "    def __init__(self, image_dir, csv_path, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "        # 1. Load CSV\n",
        "        # We assume the CSV has columns: 'image_name' and 'bbox'\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Create a dictionary for fast lookup:\n",
        "        # {'image_01.jpg': \"[[10, 10, 100, 100]]\", ...}\n",
        "        self.bbox_map = dict(zip(df['image_name'], df['bbox']))\n",
        "\n",
        "        # 2. Filter images\n",
        "        # Only include images that exist in BOTH the folder AND the CSV\n",
        "        available_files = set(os.listdir(image_dir))\n",
        "        self.images = [img for img in df['image_name'] if img in available_files]\n",
        "\n",
        "        print(f\"Found {len(self.images)} images with matching bounding boxes in CSV.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Load Image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get original dimensions for scaling calculations\n",
        "        original_H, original_W = image.shape[:2]\n",
        "\n",
        "        # Get Bounding Boxes\n",
        "        # The CSV likely stores them as strings, so we parse them into lists\n",
        "        boxes_raw = self.bbox_map[img_name]\n",
        "        if isinstance(boxes_raw, str):\n",
        "            boxes_raw = ast.literal_eval(boxes_raw)\n",
        "\n",
        "        # Resize Image to model input size (usually 1024 for SAM)\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "\n",
        "        # --- CRITICAL: Rescale Bounding Boxes ---\n",
        "        # If we resize the image, we MUST resize the box coordinates too\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes_raw:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Normalize Image\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        # Return dict (Note: No 'mask' key needed for pure inference)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-KsLOteEdwo"
      },
      "source": [
        "**Note:** Please modify the paths below to the your own paths that contain real world dataset and the csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tM39-2BA73W"
      },
      "outputs": [],
      "source": [
        "real_world_dataset_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/real_world/\"\n",
        "csv_file_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/real_world/real_world_dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK5Zvfq5BHmI",
        "outputId": "448f3ecd-5cef-4c1e-c168-580ff441e30f"
      },
      "outputs": [],
      "source": [
        "dataset = RealWorldInferenceDataset(real_world_dataset_path, csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct-jNINdExOf"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtqB3qMgeXP"
      },
      "source": [
        "### MobileSAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZJZNZ4ShSKl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0RjsKhGgy7N"
      },
      "outputs": [],
      "source": [
        "# Load the MobileSAM model\n",
        "model_type = \"vit_t\"\n",
        "checkpoint_path = \"./weights/mobile_sam.pt\"\n",
        "\n",
        "mobile_sam = sam_model_registry[model_type](checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H5bHHFTB_Fv",
        "outputId": "47a7e1b7-486a-4ea7-8982-0f2022707ff4"
      },
      "outputs": [],
      "source": [
        "# Load trained mask decoder\n",
        "decoder_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/mask_decoder.pth\"\n",
        "\n",
        "# Capture the loading message\n",
        "msg = mobile_sam.mask_decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
        "\n",
        "# Check for issues\n",
        "if len(msg.missing_keys) == 0 and len(msg.unexpected_keys) == 0:\n",
        "    print(\"Mask Decoder: All weights loaded successfully with no mismatches.\")\n",
        "else:\n",
        "    print(\"Mask Decoder Load Warning:\")\n",
        "    print(f\"  Missing Keys: {msg.missing_keys}\")\n",
        "    print(f\"  Unexpected Keys: {msg.unexpected_keys}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94MjrjrCCK-",
        "outputId": "bc93276d-a68e-4c48-880d-81a49c1ff634"
      },
      "outputs": [],
      "source": [
        "# Load LoRA into the image encoder\n",
        "lora_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/lora_image_encoder\"\n",
        "mobile_sam.image_encoder = PeftModel.from_pretrained(mobile_sam.image_encoder, lora_path)\n",
        "\n",
        "# Verification Steps:\n",
        "# 1. Check if an adapter is active\n",
        "active_adapters = mobile_sam.image_encoder.active_adapters\n",
        "print(f\"Active LoRA Adapters: {active_adapters}\")\n",
        "\n",
        "# 2. Check for missing keys (PEFT models often warn during .from_pretrained)\n",
        "# If you want to be 100% sure, check if the lora layers exist in the modules\n",
        "has_lora = any(\"lora_\" in name for name, _ in mobile_sam.image_encoder.named_modules())\n",
        "if has_lora:\n",
        "    print(\"LoRA layers detected in the Image Encoder.\")\n",
        "else:\n",
        "    print(\"ERROR: No LoRA layers found. The adapter was not applied correctly.\")\n",
        "\n",
        "# 3. Print trainable parameters (should be 0 for inference, but confirms structure)\n",
        "mobile_sam.image_encoder.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eYBkcvk_hJtH",
        "outputId": "700dd027-a391-4a73-8cc1-00186843ebc1"
      },
      "outputs": [],
      "source": [
        "mobile_sam.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r02lL1z0hLnz"
      },
      "source": [
        "### Finetuner Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkKtxmhzhNsF"
      },
      "outputs": [],
      "source": [
        "class MobileSAMFineTuner(nn.Module):\n",
        "    def __init__(self, sam_model):\n",
        "        super().__init__()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def forward(self, images: torch.Tensor, bboxes: list):\n",
        "        # images: [B, 3, 1024, 1024]\n",
        "        # bboxes: List of tensors, where bboxes[i] is [N_boxes, 4]\n",
        "\n",
        "        _, _, H, W = images.shape\n",
        "\n",
        "        # 1. Compute Image Embeddings (Run once per image)\n",
        "        image_embeddings = self.sam.image_encoder(images)\n",
        "        dense_pe = self.sam.prompt_encoder.get_dense_pe()\n",
        "\n",
        "        # Prepare lists to match the \"Previous Wrapper\" return format\n",
        "        final_masks_list = []\n",
        "        iou_preds_list = []\n",
        "\n",
        "        B = len(bboxes)\n",
        "\n",
        "        for i in range(B):\n",
        "            curr_box = bboxes[i] # Shape [N, 4]\n",
        "\n",
        "            # Safety check for images with no boxes\n",
        "            if curr_box.shape[0] == 0:\n",
        "                 # Return empty tensors so the list index stays aligned\n",
        "                 final_masks_list.append(torch.zeros(0, 1, H, W, device=images.device))\n",
        "                 iou_preds_list.append(torch.zeros(0, 1, device=images.device))\n",
        "                 continue\n",
        "\n",
        "            curr_embedding = image_embeddings[i].unsqueeze(0)\n",
        "\n",
        "            # Prompt encoder (Handles N boxes)\n",
        "            sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=curr_box,\n",
        "                masks=None,\n",
        "            )\n",
        "\n",
        "            # Mask decoder\n",
        "            low_res_masks, iou_predictions = self.sam.mask_decoder(\n",
        "                image_embeddings=curr_embedding,\n",
        "                image_pe=dense_pe,\n",
        "                sparse_prompt_embeddings=sparse_embeddings,\n",
        "                dense_prompt_embeddings=dense_embeddings,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "            # low_res_masks shape: [N, 1, 256, 256]\n",
        "\n",
        "            # Upsample NOW (per image) instead of stacking first\n",
        "            upsampled_masks = F.interpolate(\n",
        "                low_res_masks,\n",
        "                size=(H, W),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            # upsampled_masks shape: [N, 1, 1024, 1024]\n",
        "\n",
        "            final_masks_list.append(upsampled_masks)\n",
        "            iou_preds_list.append(iou_predictions)\n",
        "\n",
        "        # Return LISTS, not stacked tensors.\n",
        "        # The evaluation loop will access [0] to get the tensor for the first image.\n",
        "        return final_masks_list, iou_preds_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duu8C8-_hPq8"
      },
      "outputs": [],
      "source": [
        "finetuner = MobileSAMFineTuner(sam_model=mobile_sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NIRblBl3hfEr",
        "outputId": "b864dac7-61aa-44cc-aa26-5fa6d7871aa2"
      },
      "outputs": [],
      "source": [
        "finetuner.to(device)\n",
        "finetuner.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WbLGatU-u1C"
      },
      "source": [
        "## Tissue Classification Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igyvRBRXEB2t"
      },
      "outputs": [],
      "source": [
        "class KMeansClusterer:\n",
        "    def __init__(self, max_clusters: int = 3, sample_size: int = 50000, l_weight: float = 0.3):\n",
        "        self.max_clusters = max_clusters\n",
        "        self.sample_size = sample_size\n",
        "        self.l_weight = l_weight\n",
        "\n",
        "        # Reference Colors (L, A, B)\n",
        "        self.REF_LAB = np.array([\n",
        "            [20, 128, 128],    # Necrotic (Idx 0)\n",
        "            [130, 170, 130],   # Granulation (Idx 1)\n",
        "            [200, 128, 170]    # Slough (Idx 2)\n",
        "        ])\n",
        "\n",
        "        self.REF_NAMES = ['Necrotic', 'Granulation', 'Slough']\n",
        "\n",
        "        self.TISSUE_MAP = {\n",
        "          'Slough': {'id': 1, 'color': [0, 255, 255]},      # Yellow (Green + Red)\n",
        "          'Granulation': {'id': 2, 'color': [0, 0, 255]},   # Red\n",
        "          'Necrotic': {'id': 3, 'color': [0, 0, 0]},       # Black\n",
        "        }\n",
        "\n",
        "    def _preprocess_features(self, lab_pixels):\n",
        "        # Convert the image data from integers (uint8, 0-255) to floating point numbers (float32)\n",
        "        pixels = lab_pixels.astype(np.float32)\n",
        "\n",
        "        # Select all rows for 0th column, then multiply the value in that column by the l_weight\n",
        "        # l_weight is the shadow suppression factor --> how important the L channel is\n",
        "        pixels[:, 0] *= self.l_weight\n",
        "        return pixels\n",
        "\n",
        "    def cluster(self, wound_image: np.ndarray, mask: np.ndarray):\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(\" [START] K-Means Clustering Process\")\n",
        "        print(\"=\"*40)\n",
        "\n",
        "        # 1. Preprocessing\n",
        "        if mask.ndim == 3:\n",
        "            mask = mask[:, :, 0]    # Select all rows & columns but only the 0th color channel\n",
        "\n",
        "        blurred_img = cv2.GaussianBlur(wound_image, (5, 5), 0)      # Smooth the sparkles (tiny white dots from flash reflection) out\n",
        "        lab_image = cv2.cvtColor(blurred_img, cv2.COLOR_RGB2Lab)    # Convert to LAB\n",
        "\n",
        "        wound_indices = np.where(mask > 0)          # Find the coordinates (y, x) of every white pixel (where the wound is)\n",
        "        wound_pixels = lab_image[wound_indices]     # Extract the wound region out\n",
        "        n_pixels = wound_pixels.shape[0]            # Original image: (100, 100, 3), wound_pixels: (1000, 3)\n",
        "\n",
        "        print(f\"[STEP 1] Data Extraction\")\n",
        "        print(f\"  > Total Wound Pixels: {n_pixels}\")\n",
        "\n",
        "        if n_pixels == 0:\n",
        "            return wound_image\n",
        "\n",
        "        # 2. Weighting\n",
        "        weighted_pixels = self._preprocess_features(wound_pixels)\n",
        "\n",
        "        # 3. Sampling\n",
        "        if n_pixels > self.sample_size:\n",
        "            indices = np.random.choice(n_pixels, self.sample_size, replace=False)\n",
        "            training_data = weighted_pixels[indices]\n",
        "            print(f\"  > Sampling: Reduced {n_pixels} -> {self.sample_size} pixels for training.\")\n",
        "        else:\n",
        "            training_data = weighted_pixels\n",
        "            print(f\"  > Sampling: Using all {n_pixels} pixels.\")\n",
        "\n",
        "        # 4. Model Selection\n",
        "        best_kmeans = None\n",
        "        best_score = -1.0\n",
        "        best_k = 1\n",
        "\n",
        "        # Variance check on A & B channel\n",
        "        chromatic_var = np.var(training_data[:, 1:], axis=0).sum()\n",
        "        print(f\"\\n[STEP 2] Variance Check\")\n",
        "        print(f\"  > Chromatic Variance (A+B): {chromatic_var:.2f}\")\n",
        "\n",
        "        if chromatic_var > 95.0:\n",
        "            print(f\"  > Variance is high enough. Testing K=2 to K={self.max_clusters}...\")\n",
        "\n",
        "            # Finding Best K (2 or 3)\n",
        "            for k in range(2, self.max_clusters + 1):\n",
        "                kmeans = MiniBatchKMeans(n_clusters=k, batch_size=256, random_state=42, n_init=3)\n",
        "                labels = kmeans.fit_predict(training_data)\n",
        "\n",
        "                try:\n",
        "                    score = silhouette_score(training_data, labels, sample_size=1000)\n",
        "                except ValueError:\n",
        "                    score = 0\n",
        "\n",
        "                print(f\"    [TEST] K={k} -> Silhouette Score: {score:.4f}\")\n",
        "\n",
        "                # SELECTION LOGIC DEBUGGING\n",
        "                if score > 0.25:\n",
        "                    if score > best_score:\n",
        "                        print(f\"       -> ACCEPTED. (Reason: {score:.4f} >= {best_score:.4f})\")\n",
        "                        best_score = score\n",
        "                        best_kmeans = kmeans\n",
        "                        best_k = k\n",
        "                    else:\n",
        "                        print(f\"       -> REJECTED. (Reason: {score:.4f} < {best_score:.4f})\")\n",
        "                else:\n",
        "                    print(f\"       -> REJECTED. (Score too low)\")\n",
        "\n",
        "        # Fallback\n",
        "        if best_kmeans is None:\n",
        "            print(\"\\n[RESULT] Variance/Score too low. Fallback to K=1.\")\n",
        "            best_kmeans = MiniBatchKMeans(n_clusters=1, random_state=42).fit(training_data)\n",
        "        else:\n",
        "            print(f\"\\n[RESULT] Selected Best Model: K={best_k}\")\n",
        "\n",
        "        # 5. Prediction\n",
        "        all_labels = best_kmeans.predict(weighted_pixels)       # a list of labels of every wound pixels -> [0, 1, 1, 2, ...]\n",
        "        weighted_centroids = best_kmeans.cluster_centers_\n",
        "\n",
        "        # 6. Un-weight\n",
        "        real_centroids = weighted_centroids.copy()\n",
        "        real_centroids[:, 0] /= self.l_weight\n",
        "\n",
        "        # 7. Mapping\n",
        "        print(f\"\\n[STEP 3] Mapping Clusters to Tissues\")\n",
        "        mapped_labels_flat = self._map_clusters_to_tissues(all_labels, real_centroids)\n",
        "\n",
        "        # 8. Reconstruction\n",
        "        clustered_mask = np.zeros_like(mask, dtype=np.uint8)    # Create a blank canvas the exact same size as the original photo\n",
        "        clustered_mask[wound_indices] = mapped_labels_flat      # Take the first number from the mapped_labels_flat list into first coord & so on\n",
        "\n",
        "        clustered_mask = cv2.medianBlur(clustered_mask, 5)                              # smoothing out the noise\n",
        "        clustered_mask = cv2.bitwise_and(clustered_mask, clustered_mask, mask=mask)     # boundary enforcement\n",
        "\n",
        "        # Apply the color of the tissues based on their ids\n",
        "        overlay = wound_image.copy()\n",
        "        for _, props in self.TISSUE_MAP.items():\n",
        "            overlay[clustered_mask == props['id']] = props['color']\n",
        "\n",
        "        # Add transparency (60% original photo, 40% tissues' colors)\n",
        "        blended = cv2.addWeighted(wound_image, 0.6, overlay, 0.4, 0)\n",
        "        print(\"=\"*40 + \"\\n\")\n",
        "        return blended\n",
        "\n",
        "    def _map_clusters_to_tissues(self, labels, centroids):\n",
        "        # Debug: Show raw centroid data\n",
        "        for i, c in enumerate(centroids):\n",
        "            print(f\"  > Cluster {i} Centroid (LAB): [{c[0]:.1f}, {c[1]:.1f}, {c[2]:.1f}]\")\n",
        "\n",
        "        # Shrink L by 50% during matching so Color (A&B channel) is 2x more important\n",
        "        match_weight = np.array([0.5, 1.0, 1.0])\n",
        "        w_centroids = centroids * match_weight  # Weight the Centroids\n",
        "        w_refs = self.REF_LAB * match_weight    # Weight the Reference Colors\n",
        "\n",
        "        # Calculate the minimum distance from every cluster to every reference colour\n",
        "        dists = cdist(w_centroids, w_refs, metric='euclidean')\n",
        "        closest_ref_indices = np.argmin(dists, axis=1)\n",
        "\n",
        "        print(f\"  > Initial Assignments (Indices): {closest_ref_indices}\")\n",
        "\n",
        "        # --- CONFLICT RESOLUTION ---\n",
        "        unique_assignments = np.unique(closest_ref_indices)\n",
        "\n",
        "        if len(unique_assignments) < len(centroids):\n",
        "            print(\"  > [!] CONFLICT DETECTED: Multiple clusters mapped to same tissue.\")\n",
        "\n",
        "            # Case K=2\n",
        "            if len(centroids) == 2 and closest_ref_indices[0] == closest_ref_indices[1]:\n",
        "                dup_tissue = self.REF_NAMES[closest_ref_indices[0]]\n",
        "                print(f\"    > Conflict Type: Both K=2 clusters mapped to '{dup_tissue}'\")\n",
        "\n",
        "                # Identify which cluster is Darker/ Lighter\n",
        "                # Sort the indices based on the lightness of centroid\n",
        "                if centroids[0][0] < centroids[1][0]:\n",
        "                    idx_dark = 0\n",
        "                    idx_light = 1\n",
        "                else:\n",
        "                    idx_dark = 1\n",
        "                    idx_light = 0\n",
        "\n",
        "                avg_lightness = (centroids[0][0] + centroids[1][0]) / 2.0\n",
        "                print(f\"    > Action: Splitting based on Average Lightness ({avg_lightness})\")\n",
        "\n",
        "                # Dynamic Decision based on Lightness Threshold\n",
        "                if avg_lightness < 60.0:\n",
        "                    print(f\"    -> Low Lightness detected: Splitting into Necrotic + Granulation\")\n",
        "                    closest_ref_indices[idx_dark] = 0   # Necrotic (Black)\n",
        "                    closest_ref_indices[idx_light] = 1  # Granulation (Red)\n",
        "                else:\n",
        "                    print(f\"    -> High Lightness detected. Splitting into Granulation + Slough\")\n",
        "                    closest_ref_indices[idx_dark] = 1   # Granulation (Red)\n",
        "                    closest_ref_indices[idx_light] = 2  # Slough (Yellow)\n",
        "\n",
        "            # Case K=3\n",
        "            elif len(centroids) == 3:\n",
        "                print(\"    > Conflict Type: K=3 Overlap. Running Greedy Assignment...\")\n",
        "\n",
        "                # Create 9 tuples (3 clusters x 3 tissues)\n",
        "                flat_dists = []\n",
        "                for r in range(3):\n",
        "                    for c in range(3):\n",
        "                        flat_dists.append((dists[r,c], r, c))\n",
        "\n",
        "                # Sort the tuples based on their distance (smallest first)\n",
        "                flat_dists.sort(key=lambda x: x[0])\n",
        "\n",
        "                assigned_clusters = set()\n",
        "                assigned_tissues = set()\n",
        "                new_indices = [0, 0, 0]\n",
        "\n",
        "                for d, r, c in flat_dists:\n",
        "                    if r not in assigned_clusters and c not in assigned_tissues:\n",
        "                        print(f\"      -> Assigning Cluster {r} to {self.REF_NAMES[c]} (Dist={d:.1f})\")\n",
        "                        new_indices[r] = c\n",
        "                        assigned_clusters.add(r)\n",
        "                        assigned_tissues.add(c)\n",
        "                closest_ref_indices = np.array(new_indices)\n",
        "\n",
        "        # Create the Look-Up Table (LUT)\n",
        "        print(\"  > Final Mapping:\")\n",
        "\n",
        "        # Create a small & empty array of zeroes with the clsuter's size\n",
        "        # E.g. K=3, so labels will be 0, 1, 2\n",
        "        # np.max(labels) -> 2\n",
        "        # +1 so the array's length is 3 -> lut = [0, 0, 0]\n",
        "        lut = np.zeros(int(np.max(labels)) + 1, dtype=np.uint8)\n",
        "\n",
        "        # E.g. closest_ref_indices -> [1, 2, 0] = Cluster 0 is Reference 1 (Granulation) and so on\n",
        "        # cluster_idx = 0, ref_idx = 1\n",
        "        for cluster_idx, ref_idx in enumerate[Any](closest_ref_indices):\n",
        "            tissue_name = self.REF_NAMES[ref_idx]                       # Get the tissue name from REF_NAMES\n",
        "            lut[cluster_idx] = self.TISSUE_MAP[tissue_name]['id']       # Get the id of that tissue from TISSUE_MAP\n",
        "            print(f\"    -> Cluster {cluster_idx} ==> {tissue_name}\")\n",
        "\n",
        "        # Replace every number in the labels with the value in the lut\n",
        "        return lut[labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVVnSAM1sE8"
      },
      "source": [
        "## Segmentation + Tissue Classification Masks Generation & Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGT8XyvxE7g6"
      },
      "source": [
        "**Note:** Please modify the `output_dir` below to your own desired paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKrPV3vA1wwd",
        "outputId": "ac041036-04a6-4ddf-f0c6-cfddde7f5219"
      },
      "outputs": [],
      "source": [
        "# Define where to save the masks\n",
        "output_dir_mask = \"/content/drive/MyDrive/FYP/Datasets/inference/masks/\"\n",
        "output_dir_tissue = \"/content/drive/MyDrive/FYP/Datasets/inference/tissue_classification/\"\n",
        "os.makedirs(output_dir_mask, exist_ok=True)\n",
        "os.makedirs(output_dir_tissue, exist_ok=True)\n",
        "\n",
        "print(f\"Saving the segmentation masks to {output_dir_mask}\")\n",
        "print(f\"Saving the tissue classification overlays to {output_dir_tissue}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMwdNSHF1sK"
      },
      "source": [
        "**Note:**\n",
        "- There is also some padding added (`k_size` of 20 approximately enlarged the wound mask by 10 pixels). Increase `k_size` if you want more padding.\n",
        "- The mask is resized to (224, 224) already\n",
        "- The tissue classification overlay is still the original image size and not resized to 224x224 yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB1totL7li0B",
        "outputId": "31c7c517-82f9-4d5f-92bf-2bc2e697458c"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "clusterer = KMeansClusterer()\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Wrap loader with tqdm for a progress bar\n",
        "  for batch in tqdm(dataloader):\n",
        "      images = batch[\"image\"].to(device).float()\n",
        "      bboxes = [batch[\"bboxes\"][0].to(device).float()]\n",
        "      image_name = batch[\"image_name\"][0]\n",
        "\n",
        "      # Skip if no bounding boxes are detected (SAM needs a prompt)\n",
        "      if bboxes[0].numel() == 0:\n",
        "          print(f\"Skipping {image_name}: No bounding boxes detected.\")\n",
        "          # Optional: Save a black mask instead of skipping?\n",
        "          # blank_mask = np.zeros((images.shape[2], images.shape[3]), dtype=np.uint8)\n",
        "          # cv2.imwrite(save_path, blank_mask)\n",
        "          continue\n",
        "\n",
        "\n",
        "      # ------------------------------------\n",
        "      # --- Segmentation Mask Generation ---\n",
        "      # ------------------------------------\n",
        "      pred_masks_list, _ = finetuner(images, bboxes)\n",
        "\n",
        "      # Get the mask for the first image in batch\n",
        "      pred_masks = pred_masks_list[0]\n",
        "\n",
        "      # Combine multiple masks (if multiple boxes) into one binary mask\n",
        "      # sigmoid -> threshold at 0.5 -> sum across boxes -> clip to 0/1\n",
        "      combined_pred_mask = (torch.sigmoid(pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "      combined_pred_mask = (combined_pred_mask > 0).float()\n",
        "\n",
        "      # Squeeze to shape [H, W]\n",
        "      mask_tensor = combined_pred_mask.squeeze()\n",
        "\n",
        "      # --- Convert to Image Format ---\n",
        "      # Convert tensor to numpy and scale to 0-255\n",
        "      mask_np = mask_tensor.cpu().numpy().astype(np.uint8) * 255\n",
        "\n",
        "      # --- Add Padding ---\n",
        "      k_size = 20\n",
        "      kernel = np.ones((k_size, k_size), np.uint8)\n",
        "      mask_dilated = cv2.dilate(mask_np, kernel, iterations=1)\n",
        "\n",
        "      # --- Resize to 224x224 ---\n",
        "      mask_final = cv2.resize(mask_dilated, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # --- Save the Segmentation Mask ---\n",
        "      # We change extension to .png to avoid JPEG compression artifacts on masks\n",
        "      filename_no_ext = os.path.splitext(image_name)[0]\n",
        "      save_path = os.path.join(output_dir_mask, f\"{filename_no_ext}.png\")\n",
        "\n",
        "      cv2.imwrite(save_path, mask_final)\n",
        "\n",
        "      # ------------------------------------\n",
        "      # --- Tissue Classification ---\n",
        "      # ------------------------------------\n",
        "\n",
        "      binary_mask = (torch.sigmoid(pred_masks) > 0.5).float().sum(dim=0).clamp(0, 1).squeeze().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "      # Tissue Classification (K-Means)\n",
        "      # Convert tensor image back to original RGB for K-means\n",
        "      # Undo normalization: (img * std + mean) * 255\n",
        "      raw_img = images[0].permute(1, 2, 0).cpu().numpy()\n",
        "      raw_img = (raw_img * IMAGENET_STD + IMAGENET_MEAN) * 255\n",
        "      raw_img = np.clip(raw_img, 0, 255).astype(np.uint8)\n",
        "      raw_img_bgr = cv2.cvtColor(raw_img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "      tissue_mask = clusterer.cluster(raw_img_bgr, binary_mask)\n",
        "\n",
        "      # Save Tissue Classification Results\n",
        "      if tissue_mask is not None:\n",
        "        save_path_overlay = os.path.join(output_dir_tissue, f\"{filename_no_ext}_overlay.png\")\n",
        "        cv2.imwrite(save_path_overlay, tissue_mask)\n",
        "\n",
        "print(\"Inference and Tissue Classification complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
