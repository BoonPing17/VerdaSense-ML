{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmmMNJBSfTOc",
        "outputId": "c0e31a20-7604-44de-f966-a3b237850e83"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDyAfJjWfoXf"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!mkdir -p weights\n",
        "!wget -nc https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK2RiNPPf1bZ"
      },
      "outputs": [],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHXFsJlff3a7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from mobile_sam import sam_model_registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMum6w0gHYO"
      },
      "source": [
        "## Custom Dataset classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXxJe8MWGQwG"
      },
      "source": [
        "There are two custom dataset classes defined here. Please choose the one which is suitable for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNREbkagD2g"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4__VmEYoFJta"
      },
      "source": [
        "### SegmentationDataset class\n",
        "\n",
        "This class is for the dataset where it contains the ground truth segmentation masks for bounding box(es) extraction. Just need to pass the paths that contain the images and ground truth masks. It will automatically prepare the bounding box(es) coordinates from the ground truth masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya4ccR0ugSqp"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def get_bounding_boxes_from_mask(self, mask, padding_factor=0.1, min_area_threshold=5):\n",
        "        \"\"\"Get one or multiple bounding boxes from binary mask.\"\"\"\n",
        "        _, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "        boxes = []\n",
        "        for i in range(1, num_labels):\n",
        "            x, y, w, h, area = (\n",
        "                stats[i, cv2.CC_STAT_LEFT],\n",
        "                stats[i, cv2.CC_STAT_TOP],\n",
        "                stats[i, cv2.CC_STAT_WIDTH],\n",
        "                stats[i, cv2.CC_STAT_HEIGHT],\n",
        "                stats[i, cv2.CC_STAT_AREA],\n",
        "            )\n",
        "            if area < min_area_threshold:\n",
        "                continue\n",
        "\n",
        "            pad = int(max(w, h) * padding_factor)\n",
        "            x_min, y_min = max(0, x - pad), max(0, y - pad)\n",
        "            x_max, y_max = min(mask.shape[1], x + w + pad), min(mask.shape[0], y + h + pad)\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        return boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = self.masks[idx]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Load image and mask\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        original_H, original_W = mask.shape\n",
        "        boxes = self.get_bounding_boxes_from_mask(mask)\n",
        "\n",
        "        # Resize\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "        if mask.shape[:2] != self.img_size:\n",
        "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Image normalization\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        mask_tensor = torch.from_numpy((mask > 0).astype(np.float32)).unsqueeze(0)\n",
        "\n",
        "        # Return all boxes, not just one (SAM can handle multiple)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"mask\": mask_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6nvBCXQFpLa"
      },
      "source": [
        "**Note:** Please modify the paths below to your own paths that contain the wound images and their ground truth masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgZ-xMUD1BXm"
      },
      "outputs": [],
      "source": [
        "dataset_images_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/images/\"\n",
        "dataset_masks_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/masks/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UES1v_lS1LSj"
      },
      "outputs": [],
      "source": [
        "dataset = SegmentationDataset(dataset_images_path, dataset_masks_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3fvemCDDqd"
      },
      "source": [
        "### RealWorldInferenceDataset class\n",
        "This class is for real world collected dataset that has no ground truth segmentation masks for bounding box(es) extraction. Therefore, we need to manually annotate the bounding box(es) of the wound image in PASCAL format (x1, y1, x2, y2) and record them in a csv file.\n",
        "\n",
        "About the csv file, it must contain two columns: **image_name** and **bbox**. If you have different column names, you can also modify the code below to match the column names in your csv file. Please ensure your image_name is matched and the bounding box is in the correct format: [[x1, y1, x2, y2]]. If there are two bounding boxes, the format is [[x1, y1, x2, y2], [x1, y1, x2, y2]]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-g3k6AzA1IK"
      },
      "outputs": [],
      "source": [
        "class RealWorldInferenceDataset(Dataset):\n",
        "    def __init__(self, image_dir, csv_path, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "        self.image_dir = image_dir\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "        # 1. Load CSV\n",
        "        # We assume the CSV has columns: 'image_name' and 'bbox'\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Create a dictionary for fast lookup:\n",
        "        # {'image_01.jpg': \"[[10, 10, 100, 100]]\", ...}\n",
        "        self.bbox_map = dict(zip(df['image_name'], df['bbox']))\n",
        "\n",
        "        # 2. Filter images\n",
        "        # Only include images that exist in BOTH the folder AND the CSV\n",
        "        available_files = set(os.listdir(image_dir))\n",
        "        self.images = [img for img in df['image_name'] if img in available_files]\n",
        "\n",
        "        print(f\"Found {len(self.images)} images with matching bounding boxes in CSV.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Load Image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get original dimensions for scaling calculations\n",
        "        original_H, original_W = image.shape[:2]\n",
        "\n",
        "        # Get Bounding Boxes\n",
        "        # The CSV likely stores them as strings, so we parse them into lists\n",
        "        boxes_raw = self.bbox_map[img_name]\n",
        "        if isinstance(boxes_raw, str):\n",
        "            boxes_raw = ast.literal_eval(boxes_raw)\n",
        "\n",
        "        # Resize Image to model input size (usually 1024 for SAM)\n",
        "        if image.shape[:2] != self.img_size:\n",
        "            image = cv2.resize(image, self.img_size)\n",
        "\n",
        "        # --- CRITICAL: Rescale Bounding Boxes ---\n",
        "        # If we resize the image, we MUST resize the box coordinates too\n",
        "        target_H, target_W = self.img_size\n",
        "        scale_x = target_W / original_W\n",
        "        scale_y = target_H / original_H\n",
        "\n",
        "        boxes_rescaled = []\n",
        "        for box in boxes_raw:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            boxes_rescaled.append([\n",
        "                x_min * scale_x,\n",
        "                y_min * scale_y,\n",
        "                x_max * scale_x,\n",
        "                y_max * scale_y\n",
        "            ])\n",
        "\n",
        "        # Normalize Image\n",
        "        image = image.astype(\"float32\") / 255.0\n",
        "        image = (image - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image).permute(2, 0, 1)\n",
        "\n",
        "        # Return dict (Note: No 'mask' key needed for pure inference)\n",
        "        return {\n",
        "            \"image\": image_tensor,\n",
        "            \"bboxes\": torch.tensor(boxes_rescaled, dtype=torch.float32),\n",
        "            \"image_name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-KsLOteEdwo"
      },
      "source": [
        "**Note:** Please modify the paths below to the your own paths that contain real world dataset and the csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tM39-2BA73W"
      },
      "outputs": [],
      "source": [
        "real_world_dataset_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/real_world/\"\n",
        "csv_file_path = \"/content/drive/MyDrive/FYP/Datasets/test_inference/real_world/real_world_dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK5Zvfq5BHmI"
      },
      "outputs": [],
      "source": [
        "dataset = RealWorldInferenceDataset(real_world_dataset_path, csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct-jNINdExOf"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtqB3qMgeXP"
      },
      "source": [
        "### MobileSAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZJZNZ4ShSKl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0RjsKhGgy7N"
      },
      "outputs": [],
      "source": [
        "# Load the MobileSAM model\n",
        "model_type = \"vit_t\"\n",
        "checkpoint_path = \"./weights/mobile_sam.pt\"\n",
        "\n",
        "mobile_sam = sam_model_registry[model_type](checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5bHHFTB_Fv"
      },
      "outputs": [],
      "source": [
        "# Load trained mask decoder\n",
        "decoder_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/mask_decoder.pth\"\n",
        "\n",
        "# Capture the loading message\n",
        "msg = mobile_sam.mask_decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
        "\n",
        "# Check for issues\n",
        "if len(msg.missing_keys) == 0 and len(msg.unexpected_keys) == 0:\n",
        "    print(\"Mask Decoder: All weights loaded successfully with no mismatches.\")\n",
        "else:\n",
        "    print(\"Mask Decoder Load Warning:\")\n",
        "    print(f\"  Missing Keys: {msg.missing_keys}\")\n",
        "    print(f\"  Unexpected Keys: {msg.unexpected_keys}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W94MjrjrCCK-"
      },
      "outputs": [],
      "source": [
        "# Load LoRA into the image encoder\n",
        "lora_path = \"/content/drive/MyDrive/FYP/MobileSAM_Finetuning/checkpoints/Run_20251214-093848/lora_image_encoder\"\n",
        "mobile_sam.image_encoder = PeftModel.from_pretrained(mobile_sam.image_encoder, lora_path)\n",
        "\n",
        "# Verification Steps:\n",
        "# 1. Check if an adapter is active\n",
        "active_adapters = mobile_sam.image_encoder.active_adapters\n",
        "print(f\"Active LoRA Adapters: {active_adapters}\")\n",
        "\n",
        "# 2. Check for missing keys (PEFT models often warn during .from_pretrained)\n",
        "# If you want to be 100% sure, check if the lora layers exist in the modules\n",
        "has_lora = any(\"lora_\" in name for name, _ in mobile_sam.image_encoder.named_modules())\n",
        "if has_lora:\n",
        "    print(\"LoRA layers detected in the Image Encoder.\")\n",
        "else:\n",
        "    print(\"ERROR: No LoRA layers found. The adapter was not applied correctly.\")\n",
        "\n",
        "# 3. Print trainable parameters (should be 0 for inference, but confirms structure)\n",
        "mobile_sam.image_encoder.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eYBkcvk_hJtH"
      },
      "outputs": [],
      "source": [
        "mobile_sam.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r02lL1z0hLnz"
      },
      "source": [
        "### Finetuner Wrapper Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkKtxmhzhNsF"
      },
      "outputs": [],
      "source": [
        "class MobileSAMFineTuner(nn.Module):\n",
        "    def __init__(self, sam_model):\n",
        "        super().__init__()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def forward(self, images: torch.Tensor, bboxes: list):\n",
        "        # images: [B, 3, 1024, 1024]\n",
        "        # bboxes: List of tensors, where bboxes[i] is [N_boxes, 4]\n",
        "\n",
        "        _, _, H, W = images.shape\n",
        "\n",
        "        # 1. Compute Image Embeddings (Run once per image)\n",
        "        image_embeddings = self.sam.image_encoder(images)\n",
        "        dense_pe = self.sam.prompt_encoder.get_dense_pe()\n",
        "\n",
        "        # Prepare lists to match the \"Previous Wrapper\" return format\n",
        "        final_masks_list = []\n",
        "        iou_preds_list = []\n",
        "\n",
        "        B = len(bboxes)\n",
        "\n",
        "        for i in range(B):\n",
        "            curr_box = bboxes[i] # Shape [N, 4]\n",
        "\n",
        "            # Safety check for images with no boxes\n",
        "            if curr_box.shape[0] == 0:\n",
        "                 # Return empty tensors so the list index stays aligned\n",
        "                 final_masks_list.append(torch.zeros(0, 1, H, W, device=images.device))\n",
        "                 iou_preds_list.append(torch.zeros(0, 1, device=images.device))\n",
        "                 continue\n",
        "\n",
        "            curr_embedding = image_embeddings[i].unsqueeze(0)\n",
        "\n",
        "            # Prompt encoder (Handles N boxes)\n",
        "            sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=curr_box,\n",
        "                masks=None,\n",
        "            )\n",
        "\n",
        "            # Mask decoder\n",
        "            low_res_masks, iou_predictions = self.sam.mask_decoder(\n",
        "                image_embeddings=curr_embedding,\n",
        "                image_pe=dense_pe,\n",
        "                sparse_prompt_embeddings=sparse_embeddings,\n",
        "                dense_prompt_embeddings=dense_embeddings,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "            # low_res_masks shape: [N, 1, 256, 256]\n",
        "\n",
        "            # Upsample NOW (per image) instead of stacking first\n",
        "            upsampled_masks = F.interpolate(\n",
        "                low_res_masks,\n",
        "                size=(H, W),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            # upsampled_masks shape: [N, 1, 1024, 1024]\n",
        "\n",
        "            final_masks_list.append(upsampled_masks)\n",
        "            iou_preds_list.append(iou_predictions)\n",
        "\n",
        "        # Return LISTS, not stacked tensors.\n",
        "        # The evaluation loop will access [0] to get the tensor for the first image.\n",
        "        return final_masks_list, iou_preds_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duu8C8-_hPq8"
      },
      "outputs": [],
      "source": [
        "finetuner = MobileSAMFineTuner(sam_model=mobile_sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NIRblBl3hfEr"
      },
      "outputs": [],
      "source": [
        "finetuner.to(device)\n",
        "finetuner.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVVnSAM1sE8"
      },
      "source": [
        "## Masks Generation & Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGT8XyvxE7g6"
      },
      "source": [
        "**Note:** Please modify the `output_dir` below to your own desired path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKrPV3vA1wwd"
      },
      "outputs": [],
      "source": [
        "# Define where to save the masks\n",
        "output_dir = \"/content/drive/MyDrive/FYP/Datasets/test_inference/inference_masks/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMwdNSHF1sK"
      },
      "source": [
        "**Note:**\n",
        "- There is also some padding added (`k_size` of 20 approximately enlarged the wound mask by 10 pixels). Increase `k_size` if you want more padding.\n",
        "- The mask is resized to (224, 224) already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB1totL7li0B"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Wrap loader with tqdm for a progress bar\n",
        "  for batch in tqdm(dataloader):\n",
        "      images = batch[\"image\"].to(device).float()\n",
        "      bboxes = [batch[\"bboxes\"][0].to(device).float()]\n",
        "      image_name = batch[\"image_name\"][0]\n",
        "\n",
        "      # Skip if no bounding boxes are detected (SAM needs a prompt)\n",
        "      if bboxes[0].numel() == 0:\n",
        "          print(f\"Skipping {image_name}: No bounding boxes detected.\")\n",
        "          # Optional: Save a black mask instead of skipping?\n",
        "          # blank_mask = np.zeros((images.shape[2], images.shape[3]), dtype=np.uint8)\n",
        "          # cv2.imwrite(save_path, blank_mask)\n",
        "          continue\n",
        "\n",
        "      # --- Forward pass (Generate Mask) ---\n",
        "      pred_masks_list, _ = finetuner(images, bboxes)\n",
        "\n",
        "      # Get the mask for the first image in batch\n",
        "      pred_masks = pred_masks_list[0]\n",
        "\n",
        "      # Combine multiple masks (if multiple boxes) into one binary mask\n",
        "      # sigmoid -> threshold at 0.5 -> sum across boxes -> clip to 0/1\n",
        "      combined_pred_mask = (torch.sigmoid(pred_masks) > 0.5).float().sum(dim=0, keepdim=True)\n",
        "      combined_pred_mask = (combined_pred_mask > 0).float()\n",
        "\n",
        "      # Squeeze to shape [H, W]\n",
        "      mask_tensor = combined_pred_mask.squeeze()\n",
        "\n",
        "      # --- Convert to Image Format ---\n",
        "      # Convert tensor to numpy and scale to 0-255\n",
        "      mask_np = mask_tensor.cpu().numpy().astype(np.uint8) * 255\n",
        "\n",
        "      # --- Add Padding ---\n",
        "      k_size = 20\n",
        "      kernel = np.ones((k_size, k_size), np.uint8)\n",
        "      mask_dilated = cv2.dilate(mask_np, kernel, iterations=1)\n",
        "\n",
        "      # --- Resize to 224x224 ---\n",
        "      mask_final = cv2.resize(mask_dilated, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "      # --- Save the Mask ---\n",
        "      # We change extension to .png to avoid JPEG compression artifacts on masks\n",
        "      filename_no_ext = os.path.splitext(image_name)[0]\n",
        "      save_path = os.path.join(output_dir, f\"{filename_no_ext}.png\")\n",
        "\n",
        "      cv2.imwrite(save_path, mask_final)\n",
        "      print(f\"Successfully saved the mask to {save_path}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
