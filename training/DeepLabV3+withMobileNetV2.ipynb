{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18041,
     "status": "ok",
     "timestamp": 1768626822294,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "qzmpbNPKK3RE",
    "outputId": "ae766e74-2d0b-4057-814a-a0e02fbf3891"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBqz2VbJK5sT"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10856,
     "status": "ok",
     "timestamp": 1768626838036,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "_Q_ewT-iLOMD",
    "outputId": "77d246c9-23de-4177-e284-c136248d8fe7"
   },
   "outputs": [],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16089,
     "status": "ok",
     "timestamp": 1768626854127,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "LXZuf18ECGsl",
    "outputId": "1b581e8d-e93c-4e17-d4dd-48c29dca8339"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1768626982233,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "duC-hmdvGa3w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR # learning rate scheduler\n",
    "from monai.losses import DiceCELoss\n",
    "\n",
    "# Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# TensorBoard logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Garbage collection\n",
    "import gc\n",
    "\n",
    "# To accelerate 16-bit matrix multiplication\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ_rzmw0N2BF"
   },
   "source": [
    "## Move data to local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the `zip_train_source_path` and `zip_val_source_path` to your own paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8401,
     "status": "ok",
     "timestamp": 1768626894426,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "D__2g2QsN5AG",
    "outputId": "84537236-c8a6-450a-cb77-fe94fb58df8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying /content/drive/MyDrive/VerdaSense/Dataset/zipped/train-v2.zip to /content/data\n",
      "Copying /content/drive/MyDrive/VerdaSense/Dataset/zipped/validation-v2.zip to /content/data\n",
      "Copying complete.\n"
     ]
    }
   ],
   "source": [
    "zip_train_source_path = \"/content/drive/MyDrive/VerdaSense/Dataset/zipped/train.zip\"\n",
    "zip_val_source_path = \"/content/drive/MyDrive/VerdaSense/Dataset/zipped/validation.zip\"\n",
    "\n",
    "local_data_dir = \"/content/data\"\n",
    "\n",
    "!mkdir -p \"$local_data_dir\"\n",
    "\n",
    "print(f\"Copying {zip_train_source_path} to {local_data_dir}\")\n",
    "!cp \"$zip_train_source_path\" \"$local_data_dir/\"\n",
    "\n",
    "print(f\"Copying {zip_val_source_path} to {local_data_dir}\")\n",
    "!cp \"$zip_val_source_path\" \"$local_data_dir/\"\n",
    "\n",
    "print(\"Copying complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7468,
     "status": "ok",
     "timestamp": 1768626901896,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "S0Vz3RsSN5C9",
    "outputId": "01f4eadc-bdad-4f60-c596-7511e3a05f71"
   },
   "outputs": [],
   "source": [
    "local_zip_train_path = f\"{local_data_dir}/train.zip\"\n",
    "local_zip_val_path = f\"{local_data_dir}/validation.zip\"\n",
    "\n",
    "unzip_destination_path = local_data_dir\n",
    "\n",
    "print(f\"Unzipping {local_zip_train_path} to {unzip_destination_path}\")\n",
    "!unzip -q \"$local_zip_train_path\" -d \"$unzip_destination_path\"\n",
    "\n",
    "print(f\"Unzipping {local_zip_val_path} to {unzip_destination_path}\")\n",
    "!unzip -q \"$local_zip_val_path\" -d \"$unzip_destination_path\"\n",
    "\n",
    "print(\"Unzipping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1768626901940,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "pW1THsO5N5Gn",
    "outputId": "22e02eb7-e4d4-4f1c-87e6-5bdc79fce782"
   },
   "outputs": [],
   "source": [
    "local_train_image_path = os.path.join(local_data_dir, \"train\", \"images\")\n",
    "local_train_mask_path = os.path.join(local_data_dir, \"train\", \"masks\")\n",
    "\n",
    "# Check if directories exist\n",
    "if os.path.exists(local_train_image_path):\n",
    "  num_images = len(os.listdir(local_train_image_path))\n",
    "  print(f\"Number of images in {local_train_image_path}: {num_images}\")\n",
    "else:\n",
    "  print(f\"Directory {local_train_image_path} does not exist.\")\n",
    "\n",
    "if os.path.exists(local_train_mask_path):\n",
    "  num_masks = len(os.listdir(local_train_mask_path))\n",
    "  print(f\"Number of masks in {local_train_mask_path}: {num_masks}\")\n",
    "else:\n",
    "  print(f\"Directory {local_train_mask_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1768626901977,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "Q4waUSxiN57y",
    "outputId": "3a8a99f6-edb3-42b7-a6db-962a6a5d0a42"
   },
   "outputs": [],
   "source": [
    "local_val_image_path = os.path.join(local_data_dir, \"validation\", \"images\")\n",
    "local_val_mask_path = os.path.join(local_data_dir, \"validation\", \"masks\")\n",
    "\n",
    "# Check if directories exist\n",
    "if os.path.exists(local_val_image_path):\n",
    "  num_images = len(os.listdir(local_val_image_path))\n",
    "  print(f\"Number of images in {local_val_image_path}: {num_images}\")\n",
    "else:\n",
    "  print(f\"Directory {local_val_image_path} does not exist.\")\n",
    "\n",
    "if os.path.exists(local_val_mask_path):\n",
    "  num_masks = len(os.listdir(local_val_mask_path))\n",
    "  print(f\"Number of masks in {local_val_mask_path}: {num_masks}\")\n",
    "else:\n",
    "  print(f\"Directory {local_val_mask_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80lxBFwZK7RV"
   },
   "source": [
    "## Data augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1768626901993,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "B8ccDjcdRsxq"
   },
   "outputs": [],
   "source": [
    "# Standard ImageNet Normalization\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1768626902111,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "6BnrS186Vt5m"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    # Geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.Affine(\n",
    "        scale=(0.95, 1.05),\n",
    "        translate_percent=(-0.05, 0.05),\n",
    "        rotate=(-15, 15),\n",
    "        border_mode=cv2.BORDER_REFLECT,\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    # Photometric transforms (brightness & contrast)\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.15,\n",
    "        contrast_limit=0.15,\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    # Noise & blur\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(std_range=(0.1,0.15)),  # Lower range, reduce blur\n",
    "        A.MedianBlur(blur_limit=3),\n",
    "    ], p=0.3),\n",
    "\n",
    "    # Occlusion\n",
    "    A.CoarseDropout(\n",
    "        num_holes_range = (1, 4),\n",
    "        p=0.4\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgGfP83cLEqa"
   },
   "source": [
    "## custom SegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768626902125,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "MvF0JAL2G3tC"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform # Only for geometric transformations\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        self.img_size = img_size\n",
    "        self.mean = np.array(mean)\n",
    "        self.std = np.array(std)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        mask_name = self.masks[idx]\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        # Albumentations expects a NumPy array with uint8 data type\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply the geometric transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Manual steps for resize, standardization, and conversion\n",
    "        # 1. Resize if necessary\n",
    "        if (image.shape[1], image.shape[0]) != self.img_size:\n",
    "            image = cv2.resize(image, self.img_size)\n",
    "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # 2. Convert image to float and standardize\n",
    "        image = image.astype(\"float32\") / 255.0\n",
    "        image = (image - self.mean) / self.std\n",
    "\n",
    "        # 3. Convert image to torch tensor and permute (HWC -> CHW)\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        # 4. Convert mask to float and add a channel dimension\n",
    "        mask = (mask > 0).astype(\"float32\") # Convert to binary (0.0 or 1.0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0) # HW -> 1HW\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1768626902174,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "HxMhRzTJR0s7"
   },
   "outputs": [],
   "source": [
    "# Create the dataset using the on-the-fly transformations\n",
    "train_dataset = SegmentationDataset(local_train_image_path, local_train_mask_path, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(local_val_image_path, local_val_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCGwUziJR1oo"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768626902175,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "MzFTiUBuR3a7"
   },
   "outputs": [],
   "source": [
    "def denormalize_image(image_tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    \"\"\"Reverses the normalization process for visualization.\"\"\"\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy() # CHW -> HWC\n",
    "    image_np = std * image_np + mean\n",
    "    return np.clip(image_np, 0, 1) # Clip values to be in the [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1768626902221,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "lHPC1Lb7R3dG"
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix_components(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the confusion matrix components (TP, FP, FN) for a batch.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Ground truth masks, a tensor of 0s and 1s.\n",
    "        y_pred (torch.Tensor): Binary tensor after applying the sigmoid function and threshold.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing True Positives, False Positives, False Negatives, and True Negatives.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten tensors for easier calculation\n",
    "    y_true_flat = y_true.view(-1)\n",
    "    y_pred_flat = y_pred.view(-1)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    true_positives = ((y_pred_flat == 1) & (y_true_flat == 1)).sum().item()\n",
    "    false_positives = ((y_pred_flat == 1) & (y_true_flat == 0)).sum().item()\n",
    "    false_negatives = ((y_pred_flat == 0) & (y_true_flat == 1)).sum().item()\n",
    "    true_negatives = ((y_pred_flat == 0) & (y_true_flat == 0)).sum().item()\n",
    "\n",
    "    return true_positives, false_positives, false_negatives, true_negatives\n",
    "\n",
    "\n",
    "def calculate_final_metrics(tp, fp, fn, tn, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates final metrics from accumulated confusion matrix components.\n",
    "    \"\"\"\n",
    "    # IoU\n",
    "    intersection = tp\n",
    "    union = tp + fp + fn\n",
    "    iou = intersection / (union + smooth)\n",
    "\n",
    "    # Recall (Sensitivity)\n",
    "    recall = tp / (tp + fn + smooth)\n",
    "\n",
    "    # Precision (Positive Predictive Value)\n",
    "    precision = tp / (tp + fp + smooth)\n",
    "\n",
    "    # Dice Coefficient / F1\n",
    "    dice = (2 * precision * recall) / (precision + recall + smooth)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + smooth)\n",
    "\n",
    "    return iou, dice, recall, precision, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1768626902222,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "93xIAqYvR3fc"
   },
   "outputs": [],
   "source": [
    "# To prepare masks for TensorBoard\n",
    "def prepare_mask_for_tb(image, mask):\n",
    "  \"\"\"\n",
    "  Converts a binary mask (H, W) into an RGB overlay (3, H, W)\n",
    "  so it can be displayed in TensorBoard.\n",
    "  \"\"\"\n",
    "  # Normalize image to 0-1 if it isn't already\n",
    "  if image.max() > 1: image = image / 255.0\n",
    "\n",
    "  # Ensure mask is 0 or 1\n",
    "  mask = mask.squeeze()\n",
    "\n",
    "  # Create a red overlay for the mask\n",
    "  overlay = torch.zeros_like(image)\n",
    "  overlay[0, :, :] = mask # Red channel\n",
    "\n",
    "  # Blend 70% Original Image + 30% Red Mask\n",
    "  blended = (image * 0.4) + (overlay * 0.6)\n",
    "\n",
    "  # Return blended image where mask exists, otherwise original image\n",
    "  return torch.where(mask.unsqueeze(0) > 0, blended, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1768626902236,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "LsOAOwJQR3ht"
   },
   "outputs": [],
   "source": [
    "def create_comparison_grid(images, labels, preds, mean=IMAGENET_MEAN, std=IMAGENET_STD, max_rows=None):\n",
    "  \"\"\"\n",
    "  Creates a grid: [Input | Ground Truth | Prediction]\n",
    "  Args:\n",
    "    images: Batch of normalized images [B, 3, H, W]\n",
    "    labels: Batch of GT masks [B, 1, H, W]\n",
    "    preds: Batch of predicted masks [B, 1, H, W]\n",
    "    max_rows: Limit the number of rows\n",
    "  \"\"\"\n",
    "\n",
    "  grid_images = []\n",
    "\n",
    "  images = images.cpu()\n",
    "  labels = labels.cpu()\n",
    "  preds = preds.cpu()\n",
    "\n",
    "  # Determine how many rows to process\n",
    "  batch_size = images.shape[0]\n",
    "  limit = batch_size if max_rows is None else min(batch_size, max_rows)\n",
    "\n",
    "  for i in range(limit):\n",
    "\n",
    "    # Prepare base image (We need a clean [3, H, W] tensor with values 0-1 for the mask overlays)\n",
    "    img_np_clean = denormalize_image(images[i], mean, std)\n",
    "    img_tensor_clean = torch.from_numpy(img_np_clean).permute(2, 0, 1).float()\n",
    "\n",
    "    # Column 2: Image + Ground Truth\n",
    "    img_gt_tensor = prepare_mask_for_tb(img_tensor_clean, labels[i])\n",
    "\n",
    "    # Column 3: Image + Prediction\n",
    "    img_pred_tensor = prepare_mask_for_tb(img_tensor_clean, preds[i])\n",
    "\n",
    "    # Append strictly in this order: Left, Middle, Right\n",
    "    grid_images.extend([img_tensor_clean, img_gt_tensor, img_pred_tensor])\n",
    "\n",
    "\n",
    "  # Create the grid\n",
    "  # nrow=3 tells TensorBoard to break the line after every 3rd image\n",
    "  final_grid = make_grid(grid_images, nrow=3, padding=5)\n",
    "\n",
    "  return final_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFeyXeXqLxQV"
   },
   "source": [
    "## DeepLabv3+ with MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1768626902267,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "AyHNZNOLLz-0"
   },
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Depthwise Separable Convolution, which is a depthwise convolution\n",
    "    followed by a pointwise (1x1) convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        # Calculate padding to keep spatial size same: p = (d * (k-1)) / 2\n",
    "        padding = dilation\n",
    "\n",
    "        # Depthwise convolution: Applies a separate filter to each input channel\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n",
    "        self.bn_depth = nn.BatchNorm2d(in_channels)\n",
    "        self.relu_depth = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pointwise convolution: A 1x1 convolution to mix the channels\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=bias)\n",
    "        self.bn_point = nn.BatchNorm2d(out_channels)\n",
    "        self.relu_point = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.bn_depth(x)\n",
    "        x = self.relu_depth(x)\n",
    "\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn_point(x)\n",
    "        x = self.relu_point(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1768626902284,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "nxy1eMRgL3ci"
   },
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        # 1x1 convolution branch (This is always a standard 1x1 convolution)\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Atrous separable convolution with rate=6\n",
    "        self.atrous_block6 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=6)\n",
    "\n",
    "        # Atrous separable convolution with rate=12\n",
    "        self.atrous_block12 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=12)\n",
    "\n",
    "        # Atrous separable convolution with rate=18\n",
    "        self.atrous_block18 = SeparableConv2d(in_channels, out_channels, kernel_size=3, dilation=18)\n",
    "\n",
    "        # Global Average Pooling branch\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final 1x1 convolution to fuse all 5 branches\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 5, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.atrous_block6(x)\n",
    "        x3 = self.atrous_block12(x)\n",
    "        x4 = self.atrous_block18(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "\n",
    "        x5 = F.interpolate(x5, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768626902306,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "u_S585RzL6Uf"
   },
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        backbone = models.mobilenet_v2(weights=\"DEFAULT\")\n",
    "        self.backbone = backbone.features  # Get all layers except classifier\n",
    "\n",
    "        # Modify MobileNetV2 for Output Stride 16\n",
    "        # Change stride of the 14th block (bottleneck)\n",
    "        self.backbone[14].conv[1][0].stride = (1, 1)\n",
    "\n",
    "        # Then all subsequent layers must use dilation=2 to maintain receptive field\n",
    "        for i in range (14, 19):\n",
    "          for m in self.backbone[i].modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "              # Only apply to 3x3 depthwise convs\n",
    "              if m.kernel_size == (3, 3):\n",
    "                m.dilation = (2, 2)\n",
    "                m.padding = (2, 2)\n",
    "\n",
    "        # Low-level features come from early layer (for decoder)\n",
    "        self.low_level_idx = 3\n",
    "        self.low_level_channels = 24\n",
    "\n",
    "        # ASPP expects 1280 channels from the last MobileNetV2 layer\n",
    "        self.aspp = ASPP(in_channels=1280, out_channels=256)\n",
    "\n",
    "        # Decoder\n",
    "        self.low_level_project = nn.Sequential(\n",
    "            nn.Conv2d(self.low_level_channels, 48, kernel_size=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      input_size = x.size()[2:]\n",
    "\n",
    "      # Extract low-level and high-level features\n",
    "      low_level_feat = None\n",
    "      feat = x\n",
    "      for i, layer in enumerate(self.backbone):\n",
    "          feat = layer(feat)\n",
    "          if i == self.low_level_idx:\n",
    "              low_level_feat = feat  # Save for decoder\n",
    "\n",
    "      high_level_feat = feat  # Final output of backbone (usually [B, 1280, H/32, W/32])\n",
    "\n",
    "      # ASPP on high-level features\n",
    "      x = self.aspp(high_level_feat)\n",
    "      x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "      # Decoder\n",
    "      low_level = self.low_level_project(low_level_feat)\n",
    "      x = torch.cat([x, low_level], dim=1)\n",
    "      x = self.decoder(x)\n",
    "      x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quKBHtAvMCtN"
   },
   "source": [
    "## Model Training's Configuration & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1768626902355,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "UxW5GmALMCL5",
    "outputId": "e658d584-f219-4697-8525-f6a5ee6236e4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQpnCBROVqli"
   },
   "source": [
    "### Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768627196334,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "a0cT6_JzVsZ-"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 40,\n",
    "    \"initial_lr\": 2e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"threshold\": 0.5,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"loss_function\": \"DiceCELoss\",\n",
    "    \"lr_scheduler\": \"CosineAnnealingLR\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jRv6GLEVvBt"
   },
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768627872113,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "81bd-qYtVuSZ",
    "outputId": "68da1a69-dd38-4028-cdf4-cca94d4d6cb2"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True, # Speeds up data transfer to GPU\n",
    "    drop_last = True, # Drop the last batch if it's smaller than batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768627883722,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "oto0-ReGVxEN",
    "outputId": "a984e28b-6311-4c29-a452-8730e0f05535"
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    shuffle = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in val_loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtpUf9MtVyla"
   },
   "source": [
    "### Visualizing the data first before passing into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2100,
     "status": "ok",
     "timestamp": 1768627887527,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "RVH18S-vV3OB",
    "outputId": "91718a50-dd3c-497a-84d2-1215e6e8e707"
   },
   "outputs": [],
   "source": [
    "train_images, train_labels= next(iter(train_loader))\n",
    "\n",
    "\n",
    "# Loop through every sample in the batch\n",
    "batch_size = train_images.shape[0]\n",
    "\n",
    "for index in range(batch_size):\n",
    "    # Prepare the Image using the Denormalize Helper Function\n",
    "    image = denormalize_image(train_images[index].cpu(), mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    # Prepare the Ground Truth Mask (Label)\n",
    "    # The label is (1, H, W). Convert to (H, W).\n",
    "    mask = train_labels[index].cpu().squeeze().numpy()\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot 1: Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Sample {index} | Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Subplot 2: Ground Truth Mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Check for all-black mask issue:\n",
    "    if np.all(mask == 0):\n",
    "        plt.title(f'Sample {index} | Ground Truth Mask (ALL BLACK - FAIL)', color='red')\n",
    "    else:\n",
    "        plt.title(f'Sample {index} | Ground Truth Mask (PASS)')\n",
    "\n",
    "    plt.imshow(mask, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2078,
     "status": "ok",
     "timestamp": 1768627890771,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "6GH-26YSV3QX",
    "outputId": "ecef0031-87e2-48c4-b4cd-28e40395ede6"
   },
   "outputs": [],
   "source": [
    "vis_images, vis_labels= next(iter(val_loader))\n",
    "\n",
    "\n",
    "# Loop through every sample in the batch\n",
    "batch_size = vis_images.shape[0]\n",
    "\n",
    "for index in range(batch_size):\n",
    "    # Prepare the Image using the Denormalize Helper Function\n",
    "    image = denormalize_image(vis_images[index].cpu(), mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    # Prepare the Ground Truth Mask (Label)\n",
    "    # The label is (1, H, W). Convert to (H, W).\n",
    "    mask = vis_labels[index].cpu().squeeze().numpy()\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot 1: Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Sample {index} | Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Subplot 2: Ground Truth Mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Check for all-black mask issue:\n",
    "    if np.all(mask == 0):\n",
    "        plt.title(f'Sample {index} | Ground Truth Mask (ALL BLACK - FAIL)', color='red')\n",
    "    else:\n",
    "        plt.title(f'Sample {index} | Ground Truth Mask (PASS)')\n",
    "\n",
    "    plt.imshow(mask, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1G3xqmnV6cb"
   },
   "source": [
    "### DeepLabV3+ with MobileNetV2 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1768627892742,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "TDTGrjnhV3St"
   },
   "outputs": [],
   "source": [
    "model = DeepLabV3Plus(num_classes=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1768627893415,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "x7LF4xarMJ_2",
    "outputId": "4723a3f7-67d2-4687-c877-d2cf92e231f3"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(config[\"batch_size\"], 3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768627896088,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "a_OPf_M8MKF3"
   },
   "outputs": [],
   "source": [
    "# Initialize criterion, optimizer and lr scheduler here\n",
    "criterion = DiceCELoss(to_onehot_y=False, sigmoid=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"initial_lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=config[\"min_lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpuAo4uWWeAF"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define your own desired paths where you want to save the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1768627897833,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "UjZcegGTWSSq",
    "outputId": "5632ffa6-42a3-4e08-cdbb-ea994d176320"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "run_name = \"Run_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define paths\n",
    "log_dir = f\"/content/drive/MyDrive/FYP/Model_Training/MobileNet/logs/{run_name}\"\n",
    "save_dir = f\"/content/drive/MyDrive/FYP/Model_Training/MobileNet/checkpoints/{run_name}\"\n",
    "\n",
    "# Create directories safely\n",
    "Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Now initialize your writer and paths\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "best_model_path = os.path.join(save_dir, \"best_deeplabv3_mobilenetv2.pth\")\n",
    "\n",
    "print(f\"Directories verified/created.\")\n",
    "print(f\"TensorBoard logging to: {log_dir}\")\n",
    "print(f\"Best DeepLabV3+ with MobileNetV2 model's weights will be saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10904922,
     "status": "ok",
     "timestamp": 1768638804769,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "_q3FsVBwWSUz",
    "outputId": "dbf2f65f-19a0-458d-aafc-c44f9b10ced1"
   },
   "outputs": [],
   "source": [
    "accumulation_steps = 2\n",
    "\n",
    "best_val_iou = -100\n",
    "patience = 20\n",
    "counter = 0\n",
    "threshold = config[\"threshold\"]\n",
    "\n",
    "train_losses = []\n",
    "train_ious = []\n",
    "train_dices = []\n",
    "train_recalls = []\n",
    "train_precisions = []\n",
    "train_accs = []\n",
    "\n",
    "val_losses = []\n",
    "val_ious = []\n",
    "val_dices = []\n",
    "val_recalls = []\n",
    "val_precisions = []\n",
    "val_accs = []\n",
    "\n",
    "\n",
    "# We'll use a single batch from the validation loader for visualization\n",
    "vis_images, vis_labels = next(iter(val_loader))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "  # --- Training Phase ---\n",
    "  model.train()\n",
    "\n",
    "  # Initialize training loss and confusion matrix components\n",
    "  train_running_loss = 0.0\n",
    "  train_total_tp, train_total_fp, train_total_fn, train_total_tn = 0, 0, 0, 0\n",
    "\n",
    "  # DataLoader unpacking\n",
    "  for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "    X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).float()\n",
    "\n",
    "\n",
    "    mask_logits = model(X_batch)\n",
    "    train_prob = torch.sigmoid(mask_logits)\n",
    "    # Predictions are already [B, 1, H, W]\n",
    "    train_preds = (train_prob > threshold).float()\n",
    "\n",
    "    # Loss calculation (MONAI DiceCELoss expects [B, 1, H, W] for both inputs and targets)\n",
    "    loss = criterion(mask_logits, y_batch) / accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    # Accumulate the loss (loss * batch_size)\n",
    "    train_running_loss += (loss.item() * accumulation_steps) * X_batch.size(0)\n",
    "\n",
    "    # Update the weights\n",
    "    if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    # Accumulate confusion matrix components\n",
    "    tp, fp, fn, tn = get_confusion_matrix_components(y_batch, train_preds)\n",
    "    train_total_tp += tp\n",
    "    train_total_fp += fp\n",
    "    train_total_fn += fn\n",
    "    train_total_tn += tn\n",
    "\n",
    "  # Calculate final epoch metrics\n",
    "  train_loss = train_running_loss / len(train_dataset)\n",
    "  train_iou, train_dice, train_recall, train_precision, train_acc = calculate_final_metrics(train_total_tp, train_total_fp, train_total_fn, train_total_tn)\n",
    "\n",
    "  train_losses.append(train_loss)\n",
    "  train_ious.append(train_iou)\n",
    "  train_dices.append(train_dice)\n",
    "  train_recalls.append(train_recall)\n",
    "  train_precisions.append(train_precision)\n",
    "  train_accs.append(train_acc)\n",
    "\n",
    "  # --- Validation Phase ---\n",
    "  model.eval()\n",
    "\n",
    "  # Initialize validation loss and confusion matrix components\n",
    "  val_running_loss = 0.0\n",
    "  val_total_tp, val_total_fp, val_total_fn, val_total_tn = 0, 0, 0, 0\n",
    "\n",
    "  # Variables to track the best and worst performing batches in this epoch\n",
    "  min_iou_in_epoch = float('inf')\n",
    "  max_iou_in_epoch = float('-inf')\n",
    "  # Add bbox tracking for best/worst case visualization\n",
    "  worst_batch_data, worst_batch_labels, worst_batch_preds= None, None, None\n",
    "  best_batch_data, best_batch_labels, best_batch_preds= None, None, None\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Correct DataLoader unpacking\n",
    "    for X_val_batch, y_val_batch in val_loader:\n",
    "      X_val_batch, y_val_batch = X_val_batch.to(device).float(), y_val_batch.to(device).float()\n",
    "\n",
    "\n",
    "      val_mask_logits = model(X_val_batch)\n",
    "      val_probs = torch.sigmoid(val_mask_logits)\n",
    "\n",
    "      val_loss = criterion(val_mask_logits, y_val_batch)\n",
    "      val_running_loss += val_loss.item() * X_val_batch.size(0)\n",
    "\n",
    "      # Convert to binary tensors\n",
    "      val_preds = (val_probs > threshold).float() # [B, 1, H, W]\n",
    "\n",
    "      tp, fp, fn, tn = get_confusion_matrix_components(y_val_batch, val_preds)\n",
    "      val_total_tp += tp\n",
    "      val_total_fp += fp\n",
    "      val_total_fn += fn\n",
    "      val_total_tn += tn\n",
    "\n",
    "      # Calculate the IoU for logging the best and worst batches for each epoch\n",
    "      batch_iou, _, _, _, _ = calculate_final_metrics(tp, fp, fn, tn)\n",
    "\n",
    "      # Track the best and worst batches in this epoch\n",
    "      if batch_iou < min_iou_in_epoch:\n",
    "          min_iou_in_epoch = batch_iou\n",
    "          # Store worst batch data & Move them to CPU immediately\n",
    "          worst_batch_data = X_val_batch.cpu()\n",
    "          worst_batch_labels = y_val_batch.cpu()\n",
    "          worst_batch_preds = val_preds.cpu()\n",
    "\n",
    "\n",
    "      if batch_iou > max_iou_in_epoch:\n",
    "          max_iou_in_epoch = batch_iou\n",
    "          # Store best batch data & Move them to CPU immediately\n",
    "          best_batch_data = X_val_batch.cpu()\n",
    "          best_batch_labels = y_val_batch.cpu()\n",
    "          best_batch_preds = val_preds.cpu()\n",
    "\n",
    "  # Calculate final epoch metrics\n",
    "  val_loss = val_running_loss / len(val_dataset)\n",
    "  val_iou, val_dice, val_recall, val_precision, val_acc = calculate_final_metrics(val_total_tp, val_total_fp, val_total_fn, val_total_tn)\n",
    "\n",
    "  val_losses.append(val_loss)\n",
    "  val_ious.append(val_iou)\n",
    "  val_dices.append(val_dice)\n",
    "  val_recalls.append(val_recall)\n",
    "  val_precisions.append(val_precision)\n",
    "  val_accs.append(val_acc)\n",
    "\n",
    "  # Learning rate scheduler (CosineAnnealingLR)\n",
    "  scheduler.step()\n",
    "\n",
    "  current_lr = optimizer.param_groups[0]['lr'] # Retreive current LR\n",
    "\n",
    "  # TensorBoard Logging\n",
    "  writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss}, epoch)\n",
    "  writer.add_scalars('IoU', {'Train': train_iou, 'Validation': val_iou}, epoch)\n",
    "  writer.add_scalars('Dice', {'Train': train_dice, 'Validation': val_dice}, epoch)\n",
    "  writer.add_scalars('Recall', {'Train': train_recall, 'Validation': val_recall}, epoch)\n",
    "  writer.add_scalars('Precision', {'Train': train_precision, 'Validation': val_precision}, epoch)\n",
    "  writer.add_scalars('Accuracy', {'Train': train_acc, 'Validation': val_acc}, epoch)\n",
    "  writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "\n",
    "\n",
    "  # Log the visualization every 5 epochs\n",
    "  if epoch % 5 == 0:\n",
    "    # --- Fixed 10 Predictions Visualization---\n",
    "    # Move copies to GPU for the model\n",
    "    temp_vis_images = vis_images.to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vis_mask_logits = model(temp_vis_images)\n",
    "\n",
    "    val_probs = torch.sigmoid(vis_mask_logits)\n",
    "    val_predictions = (val_probs > threshold).float()\n",
    "\n",
    "\n",
    "    # Logging the fixed batch\n",
    "    fixed_grid = create_comparison_grid(\n",
    "        vis_images, vis_labels, val_predictions, IMAGENET_MEAN, IMAGENET_STD, max_rows=vis_images.shape[0]\n",
    "    )\n",
    "    writer.add_image(\"Vis/Fixed\", fixed_grid, epoch)\n",
    "\n",
    "\n",
    "    # Logging the best batch\n",
    "    best_grid = create_comparison_grid(\n",
    "        best_batch_data, best_batch_labels, best_batch_preds, IMAGENET_MEAN, IMAGENET_STD, max_rows=best_batch_data.shape[0]\n",
    "    )\n",
    "    writer.add_image(\"Vis/Best\", best_grid, epoch)\n",
    "\n",
    "\n",
    "    # Logging the worst batch\n",
    "    worst_grid = create_comparison_grid(\n",
    "        worst_batch_data, worst_batch_labels, worst_batch_preds, IMAGENET_MEAN, IMAGENET_STD, max_rows=worst_batch_data.shape[0]\n",
    "    )\n",
    "    writer.add_image(\"Vis/Worst\", worst_grid, epoch)\n",
    "\n",
    "  print(f\"Epoch {epoch}:\")\n",
    "  print(f\"  Current LR: {current_lr:.6f}\")\n",
    "  print(f\"  Train Metrics: Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | Acc: {train_acc:.4f}\")\n",
    "  print(f\"  Val Metrics:   Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | Acc: {val_acc:.4f}\")\n",
    "  print(\"-\" * 100) # This line adds a separator\n",
    "\n",
    "  # Check validation IoU for improvement\n",
    "  if val_iou > best_val_iou:\n",
    "    best_val_iou = val_iou\n",
    "    counter = 0\n",
    "\n",
    "    # Save the best DeepLabV3+ with MobileNetV2 model's weight\n",
    "    torch.save(model.state_dict(), best_model_path)\n",
    "    print(f\"Saved new best model at IoU: {best_val_iou: .4f} to {best_model_path}\")\n",
    "  else:\n",
    "    counter += 1\n",
    "    print(f\"No improvement in Validation IoU for {counter} epoch(s)\")\n",
    "\n",
    "  if counter >= patience:\n",
    "    print(f\"Early stopping at epoch {epoch}. Best Validation IoU: {best_val_iou}\")\n",
    "    break\n",
    "\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "# End of run\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1768638805814,
     "user": {
      "displayName": "verdasense",
      "userId": "05478882796945057064"
     },
     "user_tz": -480
    },
    "id": "VFfPbkYDWSXJ",
    "outputId": "e0417fe5-d342-4ebf-ed25-f4b3bd4f3fd2"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "epochs_range = range(1, len(train_losses)+1)\n",
    "plt.figure(figsize=(15, 10)) # Increased figure size for better readability with 4 plots\n",
    "\n",
    "# Plot 1: Loss\n",
    "plt.subplot(2, 2, 1) # Changed to 2x2 grid, first plot\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss') # Added validation loss\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() # Added legend\n",
    "\n",
    "# Plot 2: IoU\n",
    "plt.subplot(2, 2, 2) # Second plot\n",
    "plt.plot(epochs_range, train_ious, label='Train IoU')\n",
    "plt.plot(epochs_range, val_ious, label='Validation IoU') # Added validation IoU\n",
    "plt.title('Training and Validation IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend() # Added legend\n",
    "\n",
    "# Plot 3: Dice Coefficient\n",
    "plt.subplot(2, 2, 3) # Third plot\n",
    "plt.plot(epochs_range, train_dices, label='Train Dice Coef')\n",
    "plt.plot(epochs_range, val_dices, label='Validation Dice Coef') # Added validation Dice\n",
    "plt.title('Training and Validation Dice Coefficient')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend() # Added legend\n",
    "\n",
    "# Plot 4: Accuracy\n",
    "plt.subplot(2, 2, 4) # Fourth plot\n",
    "plt.plot(epochs_range, train_accs, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_accs, label='Validation Accuracy') # Added validation Accuracy\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend() # Added legend\n",
    "\n",
    "plt.tight_layout() # Adjusts subplot params for a tight layout\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOM5Xtv8ugb6XyYbXcKDt+C",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
