{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we9avx6Vo3an",
        "outputId": "566610c2-b1ef-49f4-c822-81749d20804a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcbsz07ko_AA"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RgI4Z3cpAhO",
        "outputId": "5c776c84-0092-4eb6-f6dc-f07709f67523"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcC_z5BEhuOA",
        "outputId": "ab1176ec-e8ed-43e9-dec9-28a573701264"
      },
      "outputs": [],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH3FoIbqpAjh",
        "outputId": "20e2fa64-cda0-48ac-cb72-106f6b8ade58"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from torchinfo import summary\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR # learning rate scheduler\n",
        "from monai.losses import DiceCELoss\n",
        "\n",
        "# Augmentation\n",
        "import albumentations as A\n",
        "\n",
        "# TensorBoard logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Garbage collection\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8hQDgYQtkGt"
      },
      "source": [
        "### MobileSAM's libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7cQF7mZpAnw",
        "outputId": "eca87794-1957-4503-9f3a-46740d88809c"
      },
      "outputs": [],
      "source": [
        "!pip install segment-anything\n",
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!mkdir -p weights\n",
        "!wget -nc https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt -P ./weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np5oM280k3Qo",
        "outputId": "f5b172f2-4879-44d7-b17a-3a38b3f36425"
      },
      "outputs": [],
      "source": [
        "from mobile_sam import sam_model_registry # MobileSAM utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xotBa6HDpK8_"
      },
      "source": [
        "## Move data to local disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjIwmf7yYzh"
      },
      "source": [
        "**Note:** Replace the `zip_train_source_path` and `zip_val_source_path` to your own dataset paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjUyQlzvpMxf",
        "outputId": "a789ba68-c81c-45b3-de1d-be18173a97c2"
      },
      "outputs": [],
      "source": [
        "zip_train_source_path = \"/content/drive/MyDrive/FYP/Datasets/zipped/train.zip\"\n",
        "zip_val_source_path = \"/content/drive/MyDrive/FYP/Datasets/zipped/validation.zip\"\n",
        "\n",
        "local_data_dir = \"/content/data\"\n",
        "\n",
        "!mkdir -p \"$local_data_dir\"\n",
        "\n",
        "print(f\"Copying {zip_train_source_path} to {local_data_dir}\")\n",
        "!cp \"$zip_train_source_path\" \"$local_data_dir/\"\n",
        "\n",
        "print(f\"Copying {zip_val_source_path} to {local_data_dir}\")\n",
        "!cp \"$zip_val_source_path\" \"$local_data_dir/\"\n",
        "\n",
        "print(\"Copying complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdVbiyuCpON-",
        "outputId": "453d99f6-351f-475d-d566-0976a5bd4bf0"
      },
      "outputs": [],
      "source": [
        "local_zip_train_path = f\"{local_data_dir}/train.zip\"\n",
        "local_zip_val_path = f\"{local_data_dir}/validation.zip\"\n",
        "\n",
        "unzip_destination_path = local_data_dir\n",
        "\n",
        "print(f\"Unzipping {local_zip_train_path} to {unzip_destination_path}\")\n",
        "!unzip -q \"$local_zip_train_path\" -d \"$unzip_destination_path\"\n",
        "\n",
        "print(f\"Unzipping {local_zip_val_path} to {unzip_destination_path}\")\n",
        "!unzip -q \"$local_zip_val_path\" -d \"$unzip_destination_path\"\n",
        "\n",
        "print(\"Unzipping complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_9sm3yjpPqh",
        "outputId": "db4183be-7987-4446-fc77-31b3f56b63cf"
      },
      "outputs": [],
      "source": [
        "local_train_image_path = os.path.join(local_data_dir, \"train\", \"images\")\n",
        "local_train_mask_path = os.path.join(local_data_dir, \"train\", \"masks\")\n",
        "\n",
        "# Check if directories exist\n",
        "if os.path.exists(local_train_image_path):\n",
        "  num_images = len(os.listdir(local_train_image_path))\n",
        "  print(f\"Number of images in {local_train_image_path}: {num_images}\")\n",
        "else:\n",
        "  print(f\"Directory {local_train_image_path} does not exist.\")\n",
        "\n",
        "if os.path.exists(local_train_mask_path):\n",
        "  num_masks = len(os.listdir(local_train_mask_path))\n",
        "  print(f\"Number of masks in {local_train_mask_path}: {num_masks}\")\n",
        "else:\n",
        "  print(f\"Directory {local_train_mask_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfGd3s_9pRl6",
        "outputId": "be6345cd-e94f-42e3-af2e-ba261ebf6500"
      },
      "outputs": [],
      "source": [
        "local_val_image_path = os.path.join(local_data_dir, \"validation\", \"images\")\n",
        "local_val_mask_path = os.path.join(local_data_dir, \"validation\", \"masks\")\n",
        "\n",
        "# Check if directories exist\n",
        "if os.path.exists(local_val_image_path):\n",
        "  num_images = len(os.listdir(local_val_image_path))\n",
        "  print(f\"Number of images in {local_val_image_path}: {num_images}\")\n",
        "else:\n",
        "  print(f\"Directory {local_val_image_path} does not exist.\")\n",
        "\n",
        "if os.path.exists(local_val_mask_path):\n",
        "  num_masks = len(os.listdir(local_val_mask_path))\n",
        "  print(f\"Number of masks in {local_val_mask_path}: {num_masks}\")\n",
        "else:\n",
        "  print(f\"Directory {local_val_mask_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpWrJ1nTpWHx"
      },
      "source": [
        "## Data augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SreJAbIhpaEm"
      },
      "outputs": [],
      "source": [
        "# Standard ImageNet normalization (since MobileSAM image encoder was pre-trained on ImageNet)\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m7A-F_apbHX"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    # Geometric transforms\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.RandomRotate90(p=0.3),\n",
        "    A.Affine(\n",
        "        scale=(0.95, 1.05),\n",
        "        translate_percent=(-0.05, 0.05),\n",
        "        rotate=(-15, 15),\n",
        "        border_mode=cv2.BORDER_REFLECT,\n",
        "        p=0.5\n",
        "    ),\n",
        "\n",
        "    # Photometric transforms (brightness & contrast)\n",
        "    A.RandomBrightnessContrast(\n",
        "        brightness_limit=0.15,\n",
        "        contrast_limit=0.15,\n",
        "        p=0.5\n",
        "    ),\n",
        "\n",
        "    # Noise & blur\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(std_range=(0.1,0.15)),  # Lower range, reduce blur\n",
        "        A.MedianBlur(blur_limit=3),\n",
        "    ], p=0.3),\n",
        "\n",
        "    # Occlusion\n",
        "    A.CoarseDropout(\n",
        "        num_holes_range = (1, 4),\n",
        "        p=0.4\n",
        "    ),\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm7sOqu6peFo"
      },
      "source": [
        "## Custom SegmentationDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP8Dw0sxOkPH"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None, img_size=(1024, 1024), mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.img_size = img_size\n",
        "    self.mean = np.array(mean, dtype=np.float32)\n",
        "    self.std = np.array(std, dtype=np.float32)\n",
        "\n",
        "    # Pre-calculate samples (1 image -> N samples if N wounds)\n",
        "    # This handles the 1-to-Many relationship cleanly before training starts\n",
        "    self.samples = self._prepare_samples()\n",
        "\n",
        "  def _prepare_samples(self):\n",
        "    \"\"\"Pre-process all images to find all bounding boxes and create a flat list of samples\"\"\"\n",
        "    image_files = sorted(os.listdir(self.image_dir))\n",
        "    mask_files = sorted(os.listdir(self.mask_dir))\n",
        "    all_samples = []\n",
        "\n",
        "    for img_name, mask_name in zip(image_files, mask_files):\n",
        "      mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "      # Load the original mask once for bounding box calculation\n",
        "      # We assume the boxes are calculated on the original image size\n",
        "      # and then they will be scaled to the training size in __getitem__\n",
        "      mask_original = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "      if mask_original is None: continue\n",
        "\n",
        "      # Get bounding boxes on the ORIGINAL image dimensions\n",
        "      original_boxes = self.get_bounding_boxes_from_mask(mask_original, padding_factor=0.1, min_area_threshold=5)\n",
        "\n",
        "      for bbox in original_boxes:\n",
        "        all_samples.append({\n",
        "            'image_name': img_name,\n",
        "            'mask_name': mask_name,\n",
        "            'bbox_original': np.array(bbox, dtype=np.int32) # [xmin, ymin, xmax, ymax] on original size\n",
        "        })\n",
        "\n",
        "    return all_samples\n",
        "\n",
        "  # We dont need 'self' data, only input mask\n",
        "  @staticmethod\n",
        "  def get_bounding_boxes_from_mask(mask, padding_factor=0.1, min_area_threshold=5):\n",
        "    \"\"\"\n",
        "    Converts a binary segmentation mask with multiple disconnected regions\n",
        "    into a list of bounding boxes (xyxy), one for each region.\n",
        "    \"\"\"\n",
        "\n",
        "    if mask is None: return[]\n",
        "\n",
        "    H, W = mask.shape\n",
        "\n",
        "    # Ensure the mask is binary(0 or 255)\n",
        "    # Note: if the mask is read in as 0/1, change 1 to 255\n",
        "    _, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Connected Component Analysis\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, 8, cv2.CV_32S)\n",
        "\n",
        "    bounding_boxes = []\n",
        "\n",
        "    # Iterate through each component (starting from 1 to skip background)\n",
        "    for i in range(1, num_labels):\n",
        "      x = stats[i, cv2.CC_STAT_LEFT]\n",
        "      y = stats[i, cv2.CC_STAT_TOP]\n",
        "      w = stats[i, cv2.CC_STAT_WIDTH]\n",
        "      h = stats[i, cv2.CC_STAT_HEIGHT]\n",
        "      area = stats[i, cv2.CC_STAT_AREA]\n",
        "\n",
        "      if area >= min_area_threshold:\n",
        "        # Determine the padding amount\n",
        "        padding_pixels = int(max(w, h) * padding_factor)\n",
        "\n",
        "        # Calculate padded coordinates\n",
        "        x_min_padded = x - padding_pixels\n",
        "        y_min_padded = y - padding_pixels\n",
        "        x_max_padded = x + w + padding_pixels\n",
        "        y_max_padded = y + h + padding_pixels\n",
        "\n",
        "        # Constrain to Image Boundaries\n",
        "        x_min_final = max(0, x_min_padded)\n",
        "        y_min_final = max(0, y_min_padded)\n",
        "        x_max_final = min(W, x_max_padded)\n",
        "        y_max_final = min(H, y_max_padded)\n",
        "\n",
        "        # Store as integers\n",
        "        bbox = [int(x_min_final), int(y_min_final), int(x_max_final), int(y_max_final)]\n",
        "        bounding_boxes.append(bbox)\n",
        "\n",
        "    return bounding_boxes\n",
        "\n",
        "  def __len__(self):\n",
        "    # The length is the total number of (image, mask, bbox) triples\n",
        "    return len(self.samples)\n",
        "\n",
        "  @staticmethod\n",
        "  def _bbox_visible(bbox, mask, min_iou=0.1):\n",
        "      \"\"\"Check if bbox overlaps with mask area.\"\"\"\n",
        "      x_min, y_min, x_max, y_max = np.clip(np.array(bbox, dtype=np.int32), 0, mask.shape[1]-1)\n",
        "      box_mask = np.zeros_like(mask, dtype=np.uint8)\n",
        "      cv2.rectangle(box_mask, (x_min, y_min), (x_max, y_max), 1, -1)\n",
        "      intersection = np.logical_and(box_mask, mask > 0).sum()\n",
        "      union = (mask > 0).sum() + box_mask.sum() - intersection\n",
        "      iou = intersection / union if union > 0 else 0\n",
        "      return iou >= min_iou\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # Load raw data\n",
        "    sample = self.samples[idx]\n",
        "\n",
        "    img_name = sample['image_name']\n",
        "    mask_name = sample['mask_name']\n",
        "    bbox_original = sample['bbox_original'] # bounding box on original image size\n",
        "\n",
        "    image_path = os.path.join(self.image_dir, img_name)\n",
        "    mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "    # Load image and mask\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    bbox_transformed = bbox_original.astype(np.float32)\n",
        "\n",
        "    # Apply transformations (including geometric)\n",
        "    if self.transform:\n",
        "      try:\n",
        "        augmented = self.transform(\n",
        "            image=image,\n",
        "            mask=mask,\n",
        "            bboxes=[bbox_original],\n",
        "            category_ids=[0]\n",
        "        )\n",
        "        image = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        bboxes = augmented['bboxes']\n",
        "\n",
        "        # Check 1: Box lost? (e.g. cropped out)\n",
        "        if len(bboxes) == 0:\n",
        "          raise ValueError(\"Box lost\")\n",
        "\n",
        "        # Check 2: Mask empty? (Wound rotated out of frame but box edge remains)\n",
        "        if np.max(mask) == 0:\n",
        "          raise ValueError(\"Mask empty\")\n",
        "\n",
        "        bbox_transformed = np.array(bboxes[0], dtype=np.float32)\n",
        "      except ValueError:\n",
        "          # If augmentation fails (box lost/ invalid), try next sample recursively\n",
        "          return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "    # Check bounding box visibility (IoU with mask)\n",
        "    if not self._bbox_visible(bbox_transformed, mask, min_iou=0.1):\n",
        "        return self.__getitem__((idx + 1) % len(self.samples))\n",
        "\n",
        "    current_H, current_W = image.shape[:2]\n",
        "    target_H, target_W = self.img_size\n",
        "\n",
        "    # Manual Resize\n",
        "    if current_H != target_H or current_W != target_W:\n",
        "      image = cv2.resize(image, self.img_size)\n",
        "      mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Calculate scale factors based on the change from CURRENT size to TARGET size\n",
        "    scale_x = target_W / current_W\n",
        "    scale_y = target_H / current_H\n",
        "\n",
        "    bbox_resized = np.array([\n",
        "        bbox_transformed[0] * scale_x,\n",
        "        bbox_transformed[1] * scale_y,\n",
        "        bbox_transformed[2] * scale_x,\n",
        "        bbox_transformed[3] * scale_y\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    x_min, y_min, x_max, y_max = bbox_resized.astype(np.int32)\n",
        "\n",
        "    # Isolate ground truth mask\n",
        "    mask_np_binary = (mask > 0).astype(np.uint8)\n",
        "\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_np_binary, 8, cv2.CV_32S)\n",
        "\n",
        "    isolated_mask = np.zeros_like(mask_np_binary, dtype=np.float32)\n",
        "\n",
        "    # Iterate through components (start from 1 to skip background)\n",
        "    for i in range(1, num_labels):\n",
        "\n",
        "        # Use the component's center point for a simple, robust check:\n",
        "        # Check if the center of the component is inside the bounding box prompt.\n",
        "        center_x = stats[i, cv2.CC_STAT_LEFT] + stats[i, cv2.CC_STAT_WIDTH] // 2\n",
        "        center_y = stats[i, cv2.CC_STAT_TOP] + stats[i, cv2.CC_STAT_HEIGHT] // 2\n",
        "\n",
        "        # Check containment: [x_min, y_min, x_max, y_max)\n",
        "        if (x_min <= center_x < x_max) and (y_min <= center_y < y_max):\n",
        "            # This component belongs to the prompted region.\n",
        "            # Add its pixels to the isolated mask.\n",
        "            component_mask = (labels == i).astype(np.uint8)\n",
        "            isolated_mask += component_mask\n",
        "\n",
        "    # Ensure the final isolated mask is still binary [0 or 1]\n",
        "    isolated_mask = np.clip(isolated_mask, 0, 1).astype(np.float32)\n",
        "\n",
        "    # Standardization and Tensor Conversion\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = (image - self.mean) / self.std\n",
        "    image_tensor = torch.from_numpy(image).permute(2, 0, 1) #HWC -> CHW\n",
        "\n",
        "    mask_tensor = torch.from_numpy(isolated_mask).unsqueeze(0) #HW -> 1HW\n",
        "\n",
        "    bbox_tensor = torch.from_numpy(bbox_resized)\n",
        "\n",
        "\n",
        "    return image_tensor, mask_tensor, bbox_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhadkQ77p8eS"
      },
      "outputs": [],
      "source": [
        "# Create the dataset using the on-the-fly transformations\n",
        "train_dataset = SegmentationDataset(local_train_image_path, local_train_mask_path, transform=train_transform)\n",
        "val_dataset = SegmentationDataset(local_val_image_path, local_val_mask_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-BDvq8iq7zL"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgeQkYhlq-2b"
      },
      "outputs": [],
      "source": [
        "def denormalize_image(image_tensor, mean, std):\n",
        "    \"\"\"Reverses the normalization process for visualization.\"\"\"\n",
        "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy() # CHW -> HWC\n",
        "    image_np = std * image_np + mean\n",
        "    return np.clip(image_np, 0, 1) # Clip values to be in the [0, 1] range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU7UtshMjM-f"
      },
      "outputs": [],
      "source": [
        "def overlay_bbox_on_image(image_tensor, bbox_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Convert normalized tensor to RGB image and draw bbox.\n",
        "    image_tensor: [3, H, W] torch.Tensor\n",
        "    bbox_tensor: [4] torch.Tensor (x_min, y_min, x_max, y_max)\n",
        "    \"\"\"\n",
        "    # Denormalize\n",
        "    img = denormalize_image(image_tensor, mean, std)  # returns HWC, values [0,1]\n",
        "    img = (img * 255).astype(np.uint8).copy()          # convert to uint8\n",
        "\n",
        "    # Draw bounding box\n",
        "    bbox = bbox_tensor.cpu().numpy().astype(int)\n",
        "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color=(255, 0, 0), thickness=2)\n",
        "\n",
        "    return img  # HWC, RGB uint8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcYHCvDfjFAb"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix_components(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the confusion matrix components (TP, FP, FN) for a batch.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Ground truth masks, a tensor of 0s and 1s.\n",
        "        y_pred (torch.Tensor): Binary tensor after applying the sigmoid function and threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing True Positives, False Positives, False Negatives, and True Negatives.\n",
        "    \"\"\"\n",
        "\n",
        "    # Flatten tensors for easier calculation\n",
        "    y_true_flat = y_true.view(-1)\n",
        "    y_pred_flat = y_pred.view(-1)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    true_positives = ((y_pred_flat == 1) & (y_true_flat == 1)).sum().item()\n",
        "    false_positives = ((y_pred_flat == 1) & (y_true_flat == 0)).sum().item()\n",
        "    false_negatives = ((y_pred_flat == 0) & (y_true_flat == 1)).sum().item()\n",
        "    true_negatives = ((y_pred_flat == 0) & (y_true_flat == 0)).sum().item()\n",
        "\n",
        "    return true_positives, false_positives, false_negatives, true_negatives\n",
        "\n",
        "\n",
        "def calculate_final_metrics(tp, fp, fn, tn, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Calculates final metrics from accumulated confusion matrix components.\n",
        "    \"\"\"\n",
        "    # IoU\n",
        "    intersection = tp\n",
        "    union = tp + fp + fn\n",
        "    iou = intersection / (union + smooth)\n",
        "\n",
        "    # Recall (Sensitivity)\n",
        "    recall = tp / (tp + fn + smooth)\n",
        "\n",
        "    # Precision (Positive Predictive Value)\n",
        "    precision = tp / (tp + fp + smooth)\n",
        "\n",
        "    # Dice Coefficient / F1\n",
        "    dice = (2 * precision * recall) / (precision + recall + smooth)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + smooth)\n",
        "\n",
        "    return iou, dice, recall, precision, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL3Sedpry2Hr"
      },
      "outputs": [],
      "source": [
        "# To prepare masks for TensorBoard\n",
        "def prepare_mask_for_tb(image, mask):\n",
        "  \"\"\"\n",
        "  Converts a binary mask (H, W) into an RGB overlay (3, H, W)\n",
        "  so it can be displayed in TensorBoard.\n",
        "  \"\"\"\n",
        "  # Normalize image to 0-1 if it isn't already\n",
        "  if image.max() > 1: image = image / 255.0\n",
        "\n",
        "  # Ensure mask is 0 or 1\n",
        "  mask = mask.squeeze()\n",
        "\n",
        "  # Create a red overlay for the mask\n",
        "  overlay = torch.zeros_like(image)\n",
        "  overlay[0, :, :] = mask # Red channel\n",
        "\n",
        "  # Blend 70% Original Image + 30% Red Mask\n",
        "  blended = (image * 0.4) + (overlay * 0.6)\n",
        "\n",
        "  # Return blended image where mask exists, otherwise original image\n",
        "  return torch.where(mask.unsqueeze(0) > 0, blended, image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkM4v024y3P_"
      },
      "outputs": [],
      "source": [
        "def create_comparison_grid(images, bboxes, labels, preds, mean=IMAGENET_MEAN, std=IMAGENET_STD, max_rows=None):\n",
        "  \"\"\"\n",
        "  Creates a grid: [BBox Input | Ground Truth | Prediction]\n",
        "  Args:\n",
        "    images: Batch of normalized images [B, 3, H, W]\n",
        "    bboxes: Batch of bounding boxes [B, 4]\n",
        "    labels: Batch of GT masks [B, 1, H, W]\n",
        "    preds: Batch of predicted masks [B, 1, H, W]\n",
        "    max_rows: Limit the number of rows\n",
        "  \"\"\"\n",
        "\n",
        "  grid_images = []\n",
        "\n",
        "  images = images.cpu()\n",
        "  bboxes = bboxes.cpu()\n",
        "  labels = labels.cpu()\n",
        "  preds = preds.cpu()\n",
        "\n",
        "  # Determine how many rows to process\n",
        "  batch_size = images.shape[0]\n",
        "  limit = batch_size if max_rows is None else min(batch_size, max_rows)\n",
        "\n",
        "  for i in range(limit):\n",
        "\n",
        "    # Prepare base image (We need a clean [3, H, W] tensor with values 0-1 for the mask overlays)\n",
        "    img_np_clean = denormalize_image(images[i], mean, std)\n",
        "    img_tensor_clean = torch.from_numpy(img_np_clean).permute(2, 0, 1).float()\n",
        "\n",
        "    # Column 1: Image + Bounding box\n",
        "    img_bbox_np = overlay_bbox_on_image(images[i], bboxes[i], mean, std)\n",
        "    img_bbox_tensor = torch.from_numpy(img_bbox_np).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "    # Column 2: Image + Ground Truth\n",
        "    img_gt_tensor = prepare_mask_for_tb(img_tensor_clean, labels[i])\n",
        "\n",
        "    # Column 3: Image + Prediction\n",
        "    img_pred_tensor = prepare_mask_for_tb(img_tensor_clean, preds[i])\n",
        "\n",
        "    # Append strictly in this order: Left, Middle, Right\n",
        "    grid_images.extend([img_bbox_tensor, img_gt_tensor, img_pred_tensor])\n",
        "\n",
        "\n",
        "  # Create the grid\n",
        "  # nrow=3 tells TensorBoard to break the line after every 3rd image\n",
        "  final_grid = make_grid(grid_images, nrow=3, padding=5)\n",
        "\n",
        "  return final_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaPhWSizqJOi"
      },
      "source": [
        "## MobileSAM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltqhyiXRX5D1"
      },
      "outputs": [],
      "source": [
        "class DecoderAdapter(nn.Module):\n",
        "  \"\"\"\n",
        "  A bottleneck adapter module for PEFT.\n",
        "  Inserts a small trainable module into the mask decoder.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim: int, adapter_dim: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # Down-projection: from model dimension (in_dim) to a smaller adapter_dim\n",
        "    self.down = nn.Linear(in_dim, adapter_dim)\n",
        "\n",
        "    # Non-linearity\n",
        "    self.non_linearity = nn.GELU()\n",
        "\n",
        "    # Up-projection: from adapter_dim back to model dimension (in_dim)\n",
        "    self.up = nn.Linear(adapter_dim, in_dim)\n",
        "\n",
        "    # Initialize to near-zero to start\n",
        "    nn.init.normal_(self.up.weight, std=1e-4)\n",
        "    nn.init.zeros_(self.up.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # The adapter output is added to the input (residual connection)\n",
        "    return x + self.up(self.non_linearity(self.down(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6As7LFbTQAST"
      },
      "outputs": [],
      "source": [
        "def inject_domain_adapter(mask_decoder, adapter_dim=64):\n",
        "    \"\"\"\n",
        "    Inject domain adapters into SAM's mask decoder MLP blocks.\n",
        "    Properly registers each adapter as a submodule (tracked by .to(device)).\n",
        "    \"\"\"\n",
        "    adapter_idx = 0\n",
        "\n",
        "    modules = list(mask_decoder.named_modules())\n",
        "\n",
        "    for name, module in modules:\n",
        "        if isinstance(module, nn.Linear) and module.out_features == module.in_features:\n",
        "            in_dim = module.out_features\n",
        "            adapter = DecoderAdapter(in_dim, adapter_dim)\n",
        "\n",
        "            # Register adapter properly as a submodule\n",
        "            adapter_name = f\"domain_adapter_{adapter_idx}\"\n",
        "            setattr(mask_decoder, adapter_name, adapter)\n",
        "            adapter_idx += 1\n",
        "\n",
        "            # Wrap original forward\n",
        "            old_forward = module.forward\n",
        "\n",
        "            def new_forward(x, old_forward=old_forward, adapter=getattr(mask_decoder, adapter_name)):\n",
        "                return adapter(old_forward(x))\n",
        "\n",
        "            module.forward = new_forward\n",
        "\n",
        "    return mask_decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlKVfkMyi8I7",
        "outputId": "6e89f2ec-d8c5-4924-8f91-e03e01203695"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZtokMK9lMU6"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "sam_checkpoint = \"./weights/mobile_sam.pt\"\n",
        "model_type = \"vit_t\" # MobileSAM\n",
        "\n",
        "mobile_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "\n",
        "mobile_sam.mask_decoder = inject_domain_adapter(mobile_sam.mask_decoder, adapter_dim=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkzOJdOpkBLC",
        "outputId": "e5aabd7d-e400-4f60-bfe5-7832e01e05dc"
      },
      "outputs": [],
      "source": [
        "mobile_sam.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuq-0uTMqMOY"
      },
      "outputs": [],
      "source": [
        "TRAINING_IMG_SIZE = 1024\n",
        "SAM_EMBEDDING_SIZE = 1024\n",
        "\n",
        "class MobileSAMFineTuner(nn.Module):\n",
        "    # __init__ method remains the same\n",
        "    def __init__(self, sam_model, train_img_size, sam_emb_size):\n",
        "        super().__init__()\n",
        "        self.sam = sam_model\n",
        "        self.train_img_size = train_img_size\n",
        "        self.sam_emb_size = sam_emb_size\n",
        "        self.scale_factor = sam_emb_size / train_img_size\n",
        "\n",
        "        for param in self.sam.parameters():\n",
        "          param.requires_grad = False  # Freeze all SAM parameters\n",
        "\n",
        "        # Unfreeze adapters\n",
        "        for name, param in self.sam.mask_decoder.named_parameters():\n",
        "            if \"adapter\" in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, images: torch.Tensor, bboxes: torch.Tensor):\n",
        "        # images: [B, 3, H, W], bboxes: [B, 4]\n",
        "        B, C, H, W = images.shape\n",
        "\n",
        "        # Scale bboxes\n",
        "        scaled_bboxes = bboxes * self.scale_factor # [B, 4]\n",
        "\n",
        "        # --- Image Preprocessing (Iterate & Stack) ---\n",
        "        preprocessed_images = []\n",
        "        for i in range(B):\n",
        "            preprocessed_img = self.sam.preprocess(images[i])\n",
        "            preprocessed_images.append(preprocessed_img)\n",
        "        input_images = torch.stack(preprocessed_images, dim=0) # [B, C, 1024, 1024]\n",
        "\n",
        "\n",
        "        # --- Image Encoding (FAST & FROZEN) ---\n",
        "        with torch.no_grad():\n",
        "            image_embeddings = self.sam.image_encoder(input_images) # [B, 1024, 64, 64]\n",
        "\n",
        "        # --- Prompt/Mask Decoding (SLOW & TRAINABLE - MUST ITERATE) ---\n",
        "        final_mask_logits = []\n",
        "        iou_preds = []\n",
        "\n",
        "        for i in range(B):\n",
        "            # Get single sample tensors for prompts and image embedding\n",
        "            image_embedding_i = image_embeddings[i].unsqueeze(0) # [1, 1024, 64, 64]\n",
        "            box_i = scaled_bboxes[i].unsqueeze(0)                # [1, 4]\n",
        "\n",
        "            # Prompt Encoding for a single sample\n",
        "            sparse_embeddings_i, dense_embeddings_i = self.sam.prompt_encoder(\n",
        "                points=None,\n",
        "                boxes=box_i,\n",
        "                masks=None,\n",
        "            )\n",
        "\n",
        "            # Mask Decoding for a single sample\n",
        "            low_res_masks_i, iou_predictions_i = self.sam.mask_decoder(\n",
        "                image_embeddings=image_embedding_i,  # B=1 image embedding\n",
        "                image_pe=self.sam.prompt_encoder.get_dense_pe(),\n",
        "                sparse_prompt_embeddings=sparse_embeddings_i,\n",
        "                dense_prompt_embeddings=dense_embeddings_i,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "            final_mask_logits.append(low_res_masks_i)\n",
        "            iou_preds.append(iou_predictions_i)\n",
        "\n",
        "        # --- Final Stack and Upsampling ---\n",
        "        low_res_logits_stacked = torch.cat(final_mask_logits, dim=0)\n",
        "        iou_preds_stacked = torch.cat(iou_preds, dim=0)\n",
        "\n",
        "        # Upsample to [B, 1, 1024, 1024]\n",
        "        final_mask_logits_stacked = F.interpolate(\n",
        "            low_res_logits_stacked,\n",
        "            size=(self.train_img_size, self.train_img_size),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        return final_mask_logits_stacked, iou_preds_stacked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv5H17NzqMdv"
      },
      "source": [
        "## Model Training's Configuration & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqnRKUBWqRW8",
        "outputId": "d9ae6999-c6bc-477b-ff1e-3266f8db451c"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7VMST8Mqcjf"
      },
      "source": [
        "### Training configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7qXQ7j9v9Z5"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"batch_size\": 4,\n",
        "    \"epochs\": 40,\n",
        "    \"initial_lr\": 1e-3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"min_lr\": 1e-6,\n",
        "    \"threshold\": 0.5,\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"loss_function\": \"DiceCELoss\",\n",
        "    \"lr_scheduler\": \"CosineAnnealingLR\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DcIFrccp42U"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KDU-h95p3fH",
        "outputId": "f4292550-6e01-4f60-b243-8a4e850bb24f"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = config[\"batch_size\"],\n",
        "    shuffle = True,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True, # Speeds up data transfer to GPU\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY_Vet5zqGqj",
        "outputId": "2eb201aa-60f2-4616-e85d-3c3db3559e43"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = config[\"batch_size\"],\n",
        "    shuffle = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POA29X0AsyjZ"
      },
      "source": [
        "#### Visualize the data first before passing into the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gPN8-NhtV4Zq",
        "outputId": "47301fda-3e7d-42c5-80c0-4d7843119506"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels, train_bboxes = next(iter(train_loader))\n",
        "\n",
        "\n",
        "# Loop through every sample in the batch\n",
        "batch_size = train_images.shape[0]\n",
        "\n",
        "for index in range(batch_size):\n",
        "    # Prepare the Image using the Denormalize Helper Function\n",
        "    image = overlay_bbox_on_image(train_images[index].cpu(), train_bboxes[index], mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "\n",
        "    # Prepare the Ground Truth Mask (Label)\n",
        "    # The label is (1, H, W). Convert to (H, W).\n",
        "    mask = train_labels[index].cpu().squeeze().numpy()\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Subplot 1: Image with Bounding Box\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f'Sample {index} | Input Image with BBox')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Subplot 2: Ground Truth Mask\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Check for all-black mask issue:\n",
        "    if np.all(mask == 0):\n",
        "        plt.title('Sample {index} | Ground Truth Mask (ALL BLACK - FAIL)', color='red')\n",
        "    else:\n",
        "        plt.title(f'Sample {index} | Ground Truth Mask (PASS)')\n",
        "\n",
        "    plt.imshow(mask, cmap='viridis')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C_bkcRt8tG8J",
        "outputId": "c17d6448-f726-4933-d155-3f1f8a811cb0"
      },
      "outputs": [],
      "source": [
        "vis_images, vis_labels, vis_bboxes = next(iter(val_loader))\n",
        "\n",
        "\n",
        "# Loop through every sample in the batch\n",
        "batch_size = vis_images.shape[0]\n",
        "\n",
        "for index in range(batch_size):\n",
        "    # Prepare the Image using the Denormalize Helper Function\n",
        "    image = overlay_bbox_on_image(vis_images[index].cpu(), vis_bboxes[index], mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "\n",
        "    # Prepare the Ground Truth Mask (Label)\n",
        "    # The label is (1, H, W). Convert to (H, W).\n",
        "    mask = vis_labels[index].cpu().squeeze().numpy()\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Subplot 1: Image with Bounding Box\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f'Sample {index} | Input Image with BBox')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Subplot 2: Ground Truth Mask\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Check for all-black mask issue:\n",
        "    if np.all(mask == 0):\n",
        "        plt.title('Sample {index} | Ground Truth Mask (ALL BLACK - FAIL)', color='red')\n",
        "    else:\n",
        "        plt.title(f'Sample {index} | Ground Truth Mask (PASS)')\n",
        "\n",
        "    plt.imshow(mask, cmap='viridis')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXkBZSQjqo_j"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mqo5bF6qX8z"
      },
      "outputs": [],
      "source": [
        "# Initialize MobileSAM model\n",
        "model = MobileSAMFineTuner(mobile_sam, TRAINING_IMG_SIZE, SAM_EMBEDDING_SIZE).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi4Sg6zlqX_Z",
        "outputId": "574d03f6-d005-4ff0-c7c5-e476442e954a"
      },
      "outputs": [],
      "source": [
        "SUMMARY_BATCH_SIZE = config[\"batch_size\"]\n",
        "IMAGE_SIZE = 1024\n",
        "\n",
        "# Create dummy input data with BATCH_SIZE=1\n",
        "dummy_images = torch.randn(SUMMARY_BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE) # (B, C, H, W)\n",
        "dummy_bboxes = torch.randint(0, IMAGE_SIZE, (SUMMARY_BATCH_SIZE, 4)).float() # (B, 4)\n",
        "\n",
        "# Move to device\n",
        "device = model.sam.pixel_mean.device # Get device from the model itself\n",
        "dummy_images = dummy_images.to(device)\n",
        "dummy_bboxes = dummy_bboxes.to(device)\n",
        "\n",
        "# Call summary\n",
        "summary(model, input_data=[dummy_images, dummy_bboxes], device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW1ju5KhrM2g"
      },
      "outputs": [],
      "source": [
        "# Initialize criterion, optimizer and lr scheduler here\n",
        "\n",
        "criterion = DiceCELoss(to_onehot_y=False, sigmoid=True)\n",
        "\n",
        "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=config[\"initial_lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=config[\"min_lr\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd2-iyqmriZV"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUP3_Rx3zdQf"
      },
      "source": [
        "**Note:** You can replace the `log_dir` and `save_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4l02Fp6zld5",
        "outputId": "91be34b9-a429-44f3-d4bf-de64510bc282"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "run_name = \"Run_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Define paths\n",
        "log_dir = f\"/content/drive/MyDrive/FYP/Model_Training/SAM/logs/{run_name}\"\n",
        "save_dir = f\"/content/drive/MyDrive/FYP/Model_Training/SAM/checkpoints/{run_name}\"\n",
        "\n",
        "# Create directories safely\n",
        "Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Now initialize your writer and paths\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "best_model_path = os.path.join(save_dir, \"best_MobileSAMwithAdapter.pth\")\n",
        "\n",
        "print(f\"Directories verified/created.\")\n",
        "print(f\"TensorBoard logging to: {log_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEcvlTaErji9",
        "outputId": "3c7f00fa-23c2-4c51-baf3-975d79036a33"
      },
      "outputs": [],
      "source": [
        "best_val_iou = -100\n",
        "patience = 20\n",
        "counter = 0\n",
        "threshold = config[\"threshold\"]\n",
        "\n",
        "train_losses = []\n",
        "train_ious = []\n",
        "train_dices = []\n",
        "train_recalls = []\n",
        "train_precisions = []\n",
        "train_accs = []\n",
        "\n",
        "val_losses = []\n",
        "val_ious = []\n",
        "val_dices = []\n",
        "val_recalls = []\n",
        "val_precisions = []\n",
        "val_accs = []\n",
        "\n",
        "\n",
        "# We'll use a single batch from the validation loader for visualization\n",
        "vis_images, vis_labels, vis_bboxes = next(iter(val_loader))\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "  # --- Training Phase ---\n",
        "  model.train()\n",
        "\n",
        "  # Initialize training loss and confusion matrix components\n",
        "  train_running_loss = 0.0\n",
        "  train_total_tp, train_total_fp, train_total_fn, train_total_tn = 0, 0, 0, 0\n",
        "\n",
        "  # DataLoader unpacking\n",
        "  for X_batch, y_batch, bbox_batch in train_loader:\n",
        "    X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).float()\n",
        "    bbox_batch = bbox_batch.to(device).float() # Move bbox to device and ensure float\n",
        "\n",
        "    # Corrected MobileSAM model call (returns mask_logits [B, 1, H, W] and iou_preds)\n",
        "    mask_logits, _ = model(X_batch, bbox_batch)\n",
        "\n",
        "    # Loss calculation (MONAI DiceCELoss expects [B, 1, H, W] for both inputs and targets)\n",
        "    loss = criterion(mask_logits, y_batch)\n",
        "\n",
        "    # Accumulate the loss (loss * batch_size)\n",
        "    train_running_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    optimizer.zero_grad() # clear the gradients\n",
        "    loss.backward() # Backward pass\n",
        "    optimizer.step() # Update weights\n",
        "\n",
        "    # Convert to binary predictions (either 0 or 1)\n",
        "    train_probs = torch.sigmoid(mask_logits)\n",
        "    # Predictions are already [B, 1, H, W]\n",
        "    train_preds = (train_probs > threshold).float()\n",
        "\n",
        "    # Accumulate confusion matrix components\n",
        "    tp, fp, fn, tn = get_confusion_matrix_components(y_batch, train_preds)\n",
        "    train_total_tp += tp\n",
        "    train_total_fp += fp\n",
        "    train_total_fn += fn\n",
        "    train_total_tn += tn\n",
        "\n",
        "  # Calculate final epoch metrics\n",
        "  train_loss = train_running_loss / len(train_dataset)\n",
        "  train_iou, train_dice, train_recall, train_precision, train_acc = calculate_final_metrics(train_total_tp, train_total_fp, train_total_fn, train_total_tn)\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  train_ious.append(train_iou)\n",
        "  train_dices.append(train_dice)\n",
        "  train_recalls.append(train_recall)\n",
        "  train_precisions.append(train_precision)\n",
        "  train_accs.append(train_acc)\n",
        "\n",
        "  # --- Validation Phase ---\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize validation loss and confusion matrix components\n",
        "  val_running_loss = 0.0\n",
        "  val_total_tp, val_total_fp, val_total_fn, val_total_tn = 0, 0, 0, 0\n",
        "\n",
        "  # Variables to track the best and worst performing batches in this epoch\n",
        "  min_iou_in_epoch = float('inf')\n",
        "  max_iou_in_epoch = float('-inf')\n",
        "  # Add bbox tracking for best/worst case visualization\n",
        "  worst_batch_data, worst_batch_labels, worst_batch_preds, worst_batch_bboxes = None, None, None, None\n",
        "  best_batch_data, best_batch_labels, best_batch_preds, best_batch_bboxes = None, None, None, None\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Correct DataLoader unpacking\n",
        "    for X_val_batch, y_val_batch, bbox_val_batch in val_loader:\n",
        "      X_val_batch, y_val_batch = X_val_batch.to(device).float(), y_val_batch.to(device).float()\n",
        "      bbox_val_batch = bbox_val_batch.to(device).float()\n",
        "\n",
        "      val_mask_logits, _ = model(X_val_batch, bbox_val_batch)\n",
        "\n",
        "      val_loss = criterion(val_mask_logits, y_val_batch)\n",
        "      val_running_loss += val_loss.item() * X_val_batch.size(0)\n",
        "\n",
        "      # Convert to binary tensors\n",
        "      val_probs = torch.sigmoid(val_mask_logits)\n",
        "      val_preds = (val_probs > threshold).float() # [B, 1, H, W]\n",
        "\n",
        "      tp, fp, fn, tn = get_confusion_matrix_components(y_val_batch, val_preds)\n",
        "      val_total_tp += tp\n",
        "      val_total_fp += fp\n",
        "      val_total_fn += fn\n",
        "      val_total_tn += tn\n",
        "\n",
        "      # Calculate the IoU for logging the best and worst batches for each epoch\n",
        "      batch_iou, _, _, _, _ = calculate_final_metrics(tp, fp, fn, tn)\n",
        "\n",
        "      # Track the best and worst batches in this epoch\n",
        "      if batch_iou < min_iou_in_epoch:\n",
        "          min_iou_in_epoch = batch_iou\n",
        "          # Store worst batch data & Move them to CPU immediately\n",
        "          worst_batch_data = X_val_batch.cpu()\n",
        "          worst_batch_labels = y_val_batch.cpu()\n",
        "          worst_batch_preds = val_preds.cpu()\n",
        "          worst_batch_bboxes = bbox_val_batch.cpu()\n",
        "\n",
        "\n",
        "      if batch_iou > max_iou_in_epoch:\n",
        "          max_iou_in_epoch = batch_iou\n",
        "          # Store best batch data & Move them to CPU immediately\n",
        "          best_batch_data = X_val_batch.cpu()\n",
        "          best_batch_labels = y_val_batch.cpu()\n",
        "          best_batch_preds = val_preds.cpu()\n",
        "          best_batch_bboxes = bbox_val_batch.cpu()\n",
        "\n",
        "  # Calculate final epoch metrics\n",
        "  val_loss = val_running_loss / len(val_dataset)\n",
        "  val_iou, val_dice, val_recall, val_precision, val_acc = calculate_final_metrics(val_total_tp, val_total_fp, val_total_fn, val_total_tn)\n",
        "\n",
        "  val_losses.append(val_loss)\n",
        "  val_ious.append(val_iou)\n",
        "  val_dices.append(val_dice)\n",
        "  val_recalls.append(val_recall)\n",
        "  val_precisions.append(val_precision)\n",
        "  val_accs.append(val_acc)\n",
        "\n",
        "  # Learning rate scheduler (CosineAnnealingLR)\n",
        "  scheduler.step()\n",
        "\n",
        "  current_lr = optimizer.param_groups[0]['lr'] # Retreive current LR\n",
        "\n",
        "  # TensorBoard Logging\n",
        "  writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss}, epoch)\n",
        "  writer.add_scalars('IoU', {'Train': train_iou, 'Validation': val_iou}, epoch)\n",
        "  writer.add_scalars('Dice', {'Train': train_dice, 'Validation': val_dice}, epoch)\n",
        "  writer.add_scalars('Recall', {'Train': train_recall, 'Validation': val_recall}, epoch)\n",
        "  writer.add_scalars('Precision', {'Train': train_precision, 'Validation': val_precision}, epoch)\n",
        "  writer.add_scalars('Accuracy', {'Train': train_acc, 'Validation': val_acc}, epoch)\n",
        "  writer.add_scalar('Learning Rate', current_lr, epoch)\n",
        "\n",
        "\n",
        "  # Log the visualization every 5 epochs\n",
        "  if epoch % 5 == 0:\n",
        "    # --- Fixed 10 Predictions Visualization---\n",
        "    # Move copies to GPU for the model\n",
        "    temp_vis_images = vis_images.to(device)\n",
        "    temp_vis_bboxes = vis_bboxes.to(device)\n",
        "    vis_mask_logits, _ = model(temp_vis_images, temp_vis_bboxes)\n",
        "    val_predictions = torch.sigmoid(vis_mask_logits)\n",
        "    val_predictions = (val_predictions > threshold).float()\n",
        "\n",
        "\n",
        "    # Logging the fixed batch\n",
        "    fixed_grid = create_comparison_grid(\n",
        "        vis_images, vis_bboxes, vis_labels, val_predictions, IMAGENET_MEAN, IMAGENET_STD, max_rows=vis_images.shape[0]\n",
        "    )\n",
        "    writer.add_image(\"Vis/Fixed\", fixed_grid, epoch)\n",
        "\n",
        "\n",
        "    # Logging the best batch\n",
        "    best_grid = create_comparison_grid(\n",
        "        best_batch_data, best_batch_bboxes, best_batch_labels, best_batch_preds, IMAGENET_MEAN, IMAGENET_STD, max_rows=best_batch_data.shape[0]\n",
        "    )\n",
        "    writer.add_image(\"Vis/Best\", best_grid, epoch)\n",
        "\n",
        "\n",
        "    # Logging the worst batch\n",
        "    worst_grid = create_comparison_grid(\n",
        "        worst_batch_data, worst_batch_bboxes, worst_batch_labels, worst_batch_preds, IMAGENET_MEAN, IMAGENET_STD, max_rows=worst_batch_data.shape[0]\n",
        "    )\n",
        "    writer.add_image(\"Vis/Worst\", worst_grid, epoch)\n",
        "\n",
        "  print(f\"Epoch {epoch}:\")\n",
        "  print(f\"  Current LR: {current_lr:.6f}\")\n",
        "  print(f\"  Train Metrics: Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | Acc: {train_acc:.4f}\")\n",
        "  print(f\"  Val Metrics:   Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | Acc: {val_acc:.4f}\")\n",
        "  print(\"-\" * 100) # This line adds a separator\n",
        "\n",
        "  # Check validation IoU for improvement\n",
        "  if val_iou > best_val_iou:\n",
        "    best_val_iou = val_iou\n",
        "    counter = 0\n",
        "\n",
        "    # Save mask decoder weights\n",
        "    torch.save(model.state_dict(), best_model_path)\n",
        "    print(f\"Saved new best model at IoU: {best_val_iou: .4f} to {best_model_path}\")\n",
        "  else:\n",
        "    counter += 1\n",
        "    print(f\"No improvement in Validation IoU for {counter} epoch(s)\")\n",
        "\n",
        "  if counter >= patience:\n",
        "    print(f\"Early stopping at epoch {epoch}. Best Validation IoU: {best_val_iou}\")\n",
        "    break\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "# End of run\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLQ1EfH6lBnZ"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "HT2yApX8lBFb",
        "outputId": "9b87c3a6-a788-4225-99b6-9de473f1748c"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "epochs_range = range(1, len(train_losses)+1)\n",
        "plt.figure(figsize=(15, 10)) # Increased figure size for better readability with 4 plots\n",
        "\n",
        "# Plot 1: Loss\n",
        "plt.subplot(2, 2, 1) # Changed to 2x2 grid, first plot\n",
        "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss') # Added validation loss\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend() # Added legend\n",
        "\n",
        "# Plot 2: IoU\n",
        "plt.subplot(2, 2, 2) # Second plot\n",
        "plt.plot(epochs_range, train_ious, label='Train IoU')\n",
        "plt.plot(epochs_range, val_ious, label='Validation IoU') # Added validation IoU\n",
        "plt.title('Training and Validation IoU')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU')\n",
        "plt.legend() # Added legend\n",
        "\n",
        "# Plot 3: Dice Coefficient\n",
        "plt.subplot(2, 2, 3) # Third plot\n",
        "plt.plot(epochs_range, train_dices, label='Train Dice Coef')\n",
        "plt.plot(epochs_range, val_dices, label='Validation Dice Coef') # Added validation Dice\n",
        "plt.title('Training and Validation Dice Coefficient')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.legend() # Added legend\n",
        "\n",
        "# Plot 4: Accuracy\n",
        "plt.subplot(2, 2, 4) # Fourth plot\n",
        "plt.plot(epochs_range, train_accs, label='Train Accuracy')\n",
        "plt.plot(epochs_range, val_accs, label='Validation Accuracy') # Added validation Accuracy\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.legend() # Added legend\n",
        "\n",
        "plt.tight_layout() # Adjusts subplot params for a tight layout\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
